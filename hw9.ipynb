{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CSC 620 -- HA #9\n","\n","By: Mark Kim\n","\n","Adapted from\n","[dbaghern](https://www.kaggle.com/code/baghern/a-deep-dive-into-sklearn-pipelines/notebook)\n","\n","This assignment is about feature engineering and streamlining the process of\n","modeling.\n","\n","Below, we import the libraries we need and read in our training data.  This data\n","consists of sentences from horror stories and their authors.  The purpose of this notebook is\n","to predict the author of a horror story sentence given its text."]},{"cell_type":"code","execution_count":153,"metadata":{"_cell_guid":"0289247b-e183-4b83-9a4a-777c1ccd2cbc","_uuid":"282f761b71584453b7c57203edfcc8677d9738a0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>author</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>id26305</th>\n","      <td>This process, however, afforded me no means of...</td>\n","      <td>EAP</td>\n","    </tr>\n","    <tr>\n","      <th>id17569</th>\n","      <td>It never once occurred to me that the fumbling...</td>\n","      <td>HPL</td>\n","    </tr>\n","    <tr>\n","      <th>id11008</th>\n","      <td>In his left hand was a gold snuff box, from wh...</td>\n","      <td>EAP</td>\n","    </tr>\n","    <tr>\n","      <th>id27763</th>\n","      <td>How lovely is spring As we looked from Windsor...</td>\n","      <td>MWS</td>\n","    </tr>\n","    <tr>\n","      <th>id12958</th>\n","      <td>Finding nothing else, not even gold, the Super...</td>\n","      <td>HPL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                      text author\n","id                                                               \n","id26305  This process, however, afforded me no means of...    EAP\n","id17569  It never once occurred to me that the fumbling...    HPL\n","id11008  In his left hand was a gold snuff box, from wh...    EAP\n","id27763  How lovely is spring As we looked from Windsor...    MWS\n","id12958  Finding nothing else, not even gold, the Super...    HPL"]},"execution_count":153,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","df = pd.read_csv('./input/train.csv')\n","\n","df.dropna(axis=0)\n","df.set_index('id', inplace = True)\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9d72d93c-d319-4d4e-ad04-ee0d153f7fb3","_uuid":"2686ff0796119ece5ada79c8a9a3c35d323e9420"},"source":["## Preprocessing and Feature Engineering\n","\n","The function in this next cell does a little bit of text normalization \n","(removal of punctuation and capitalization), then does some feature engineering.\n","\n","The feature engineering of the original author consists of the following:\n","1. Number of characters in the sentence/text.\n","2. Number of words in the sentence/text (minus stopwords).\n","3. Average word length of words in sentence/text (minus stopwords).\n","4. Number of commas used in a sentence/text.\n","\n","Starting on line 26 of the code below, I have implemented the three additional\n","features required by the assignment:\n","1. Number of Adjectives.\n","2. Number of Nouns.\n","3. Number of Verbs."]},{"cell_type":"code","execution_count":154,"metadata":{"_cell_guid":"63e391e6-e238-454f-b93c-b2f249d16efc","_uuid":"2596bb126e8ee018a6940ae31c8aa89cc0d84e8d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>author</th>\n","      <th>processed</th>\n","      <th>length</th>\n","      <th>words</th>\n","      <th>words_not_stopword</th>\n","      <th>avg_word_length</th>\n","      <th>commas</th>\n","      <th>pos</th>\n","      <th>adj_count</th>\n","      <th>noun_count</th>\n","      <th>verb_count</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>id26305</th>\n","      <td>This process, however, afforded me no means of...</td>\n","      <td>EAP</td>\n","      <td>this process however afforded me no means of a...</td>\n","      <td>224</td>\n","      <td>41</td>\n","      <td>21</td>\n","      <td>6.380952</td>\n","      <td>4</td>\n","      <td>[(this, DT), (process, NN), (however, RB), (af...</td>\n","      <td>2</td>\n","      <td>12</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>id17569</th>\n","      <td>It never once occurred to me that the fumbling...</td>\n","      <td>HPL</td>\n","      <td>it never once occurred to me that the fumbling...</td>\n","      <td>70</td>\n","      <td>14</td>\n","      <td>6</td>\n","      <td>6.166667</td>\n","      <td>0</td>\n","      <td>[(it, PRP), (never, RB), (once, RB), (occurred...</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>id11008</th>\n","      <td>In his left hand was a gold snuff box, from wh...</td>\n","      <td>EAP</td>\n","      <td>in his left hand was a gold snuff box from whi...</td>\n","      <td>195</td>\n","      <td>36</td>\n","      <td>19</td>\n","      <td>5.947368</td>\n","      <td>4</td>\n","      <td>[(in, IN), (his, PRP$), (left, JJ), (hand, NN)...</td>\n","      <td>5</td>\n","      <td>10</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>id27763</th>\n","      <td>How lovely is spring As we looked from Windsor...</td>\n","      <td>MWS</td>\n","      <td>how lovely is spring as we looked from windsor...</td>\n","      <td>202</td>\n","      <td>34</td>\n","      <td>21</td>\n","      <td>6.476190</td>\n","      <td>3</td>\n","      <td>[(how, WRB), (lovely, RB), (is, VBZ), (spring,...</td>\n","      <td>6</td>\n","      <td>10</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>id12958</th>\n","      <td>Finding nothing else, not even gold, the Super...</td>\n","      <td>HPL</td>\n","      <td>finding nothing else not even gold the superin...</td>\n","      <td>170</td>\n","      <td>27</td>\n","      <td>16</td>\n","      <td>7.187500</td>\n","      <td>2</td>\n","      <td>[(finding, VBG), (nothing, NN), (else, RB), (n...</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                      text author  \\\n","id                                                                  \n","id26305  This process, however, afforded me no means of...    EAP   \n","id17569  It never once occurred to me that the fumbling...    HPL   \n","id11008  In his left hand was a gold snuff box, from wh...    EAP   \n","id27763  How lovely is spring As we looked from Windsor...    MWS   \n","id12958  Finding nothing else, not even gold, the Super...    HPL   \n","\n","                                                 processed  length  words  \\\n","id                                                                          \n","id26305  this process however afforded me no means of a...     224     41   \n","id17569  it never once occurred to me that the fumbling...      70     14   \n","id11008  in his left hand was a gold snuff box from whi...     195     36   \n","id27763  how lovely is spring as we looked from windsor...     202     34   \n","id12958  finding nothing else not even gold the superin...     170     27   \n","\n","         words_not_stopword  avg_word_length  commas  \\\n","id                                                     \n","id26305                  21         6.380952       4   \n","id17569                   6         6.166667       0   \n","id11008                  19         5.947368       4   \n","id27763                  21         6.476190       3   \n","id12958                  16         7.187500       2   \n","\n","                                                       pos  adj_count  \\\n","id                                                                      \n","id26305  [(this, DT), (process, NN), (however, RB), (af...          2   \n","id17569  [(it, PRP), (never, RB), (once, RB), (occurred...          1   \n","id11008  [(in, IN), (his, PRP$), (left, JJ), (hand, NN)...          5   \n","id27763  [(how, WRB), (lovely, RB), (is, VBZ), (spring,...          6   \n","id12958  [(finding, VBG), (nothing, NN), (else, RB), (n...          1   \n","\n","         noun_count  verb_count  \n","id                               \n","id26305          12           6  \n","id17569           2           2  \n","id11008          10           4  \n","id27763          10           5  \n","id12958           6           6  "]},"execution_count":154,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","from nltk.corpus import stopwords\n","from nltk.tag import pos_tag\n","\n","stopWords = set(stopwords.words('english'))\n","\n","#creating a function to encapsulate preprocessing, to mkae it easy to replicate on  submission data\n","def processing(df):\n","    #lowering and removing punctuation\n","    df['processed'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))\n","    \n","    #numerical feature engineering\n","    #total length of sentence\n","    df['length'] = df['processed'].apply(lambda x: len(x))\n","    #get number of words\n","    df['words'] = df['processed'].apply(lambda x: len(x.split(' ')))\n","    df['words_not_stopword'] = df['processed'].apply(\n","        lambda x: len([t for t in x.split(' ') if t not in stopWords]))\n","    #get the average word length\n","    df['avg_word_length'] = df['processed'].apply(\n","        lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopWords]) \n","            if len([len(t) for t in x.split(' ') if t not in stopWords]) > 0 else 0)\n","    #get the average word length\n","    df['commas'] = df['text'].apply(lambda x: x.count(','))\n","\n","    # This section adds the three new features as required in the assignment instructions\n","    # tokenize and tag parts of speech for processing\n","    df['pos'] = df['processed'].apply(lambda x: x.split(' ')).apply(lambda x: pos_tag(x))\n","    # get the count of adjectives\n","    adj = df['pos'].apply(\n","        lambda x: list(filter(lambda y: y[1] == 'JJ' \n","        or y[1] == 'JJR' \n","        or y[1] == 'JJS', x)))\n","    df['adj_count'] = adj.apply(lambda x: len(x))\n","    # get the count of nouns\n","    noun = df['pos'].apply(\n","        lambda x: list(filter(lambda y: y[1] == 'NN' \n","        or y[1] == 'NNS' \n","        or y[1] == 'NNP'\n","        or y[1] == 'NNPS', x)))\n","    df['noun_count'] = noun.apply(lambda x: len(x))\n","    # get the count of verbs\n","    verb = df['pos'].apply(\n","        lambda x: list(filter(lambda y: y[1] == 'VB' \n","        or y[1] == 'VBD' \n","        or y[1] == 'VBG'\n","        or y[1] == 'VBN' \n","        or y[1] == 'VBP'\n","        or y[1] == 'VBZ', x)))\n","    df['verb_count'] = verb.apply(lambda x: len(x))\n","\n","    return(df)\n","\n","df = processing(df)\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Further split the `test.csv` set in to train and test sets.\n","\n","Here we get extract the feature columns from the dataframe (with one that has\n","all features and another that only has numeric features).\n","\n","Then the set is split into train and test sets."]},{"cell_type":"code","execution_count":155,"metadata":{"_cell_guid":"27efeecb-8feb-41c1-96d8-7bc21b452e7b","_uuid":"23b0a8d116caf454325f63af4b108e74023ca8de"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>processed</th>\n","      <th>length</th>\n","      <th>words</th>\n","      <th>words_not_stopword</th>\n","      <th>avg_word_length</th>\n","      <th>commas</th>\n","      <th>adj_count</th>\n","      <th>noun_count</th>\n","      <th>verb_count</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>id19417</th>\n","      <td>this panorama is indeed glorious and i should ...</td>\n","      <td>91</td>\n","      <td>18</td>\n","      <td>6</td>\n","      <td>6.666667</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>id09522</th>\n","      <td>there was a simple natural earnestness about h...</td>\n","      <td>240</td>\n","      <td>44</td>\n","      <td>18</td>\n","      <td>6.277778</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>8</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>id22732</th>\n","      <td>who are you pray that i duc de lomelette princ...</td>\n","      <td>387</td>\n","      <td>74</td>\n","      <td>38</td>\n","      <td>5.552632</td>\n","      <td>9</td>\n","      <td>3</td>\n","      <td>18</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>id10351</th>\n","      <td>he had gone in the carriage to the nearest tow...</td>\n","      <td>118</td>\n","      <td>24</td>\n","      <td>11</td>\n","      <td>5.363636</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>id24580</th>\n","      <td>there is no method in their proceedings beyond...</td>\n","      <td>71</td>\n","      <td>13</td>\n","      <td>5</td>\n","      <td>7.000000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                 processed  length  words  \\\n","id                                                                          \n","id19417  this panorama is indeed glorious and i should ...      91     18   \n","id09522  there was a simple natural earnestness about h...     240     44   \n","id22732  who are you pray that i duc de lomelette princ...     387     74   \n","id10351  he had gone in the carriage to the nearest tow...     118     24   \n","id24580  there is no method in their proceedings beyond...      71     13   \n","\n","         words_not_stopword  avg_word_length  commas  adj_count  noun_count  \\\n","id                                                                            \n","id19417                   6         6.666667       1          1           4   \n","id09522                  18         6.277778       4          7           8   \n","id22732                  38         5.552632       9          3          18   \n","id10351                  11         5.363636       0          1           8   \n","id24580                   5         7.000000       1          0           4   \n","\n","         verb_count  \n","id                   \n","id19417           2  \n","id09522           7  \n","id22732          10  \n","id10351           3  \n","id24580           1  "]},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import train_test_split\n","\n","features= [c for c in df.columns.values if c  not in ['id','text','author','pos']]\n","numeric_features= [c for c in df.columns.values if c  not in ['id','text','author','processed','pos']]\n","target = 'author'\n","\n","X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.33, random_state=42)\n","X_train.head()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9f65b065-59e9-42ea-a76e-9a38a9e69109","_uuid":"e477b8cbf9fc8a15c9f4aa13908314350caaa95b"},"source":["### Creating a Pipeline\n","\n","First, we create functions that will return a single column from a dataframe as\n","a Pandas Series.  Apparently, the original author ran into problems between\n","trying to select a text column versus a numerical column, so a separate class\n","was created for each."]},{"cell_type":"code","execution_count":156,"metadata":{"_cell_guid":"66f0363d-1414-4d23-87fa-a0190c0f6a3a","_uuid":"2d8983e1d86a9d1323b0bde3083c6fe2e2650378","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class TextSelector(BaseEstimator, TransformerMixin):\n","    \"\"\"\n","    Transformer to select a single column from the data frame to perform additional transformations on\n","    Use on text columns in the data\n","    \"\"\"\n","    def __init__(self, key):\n","        self.key = key\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return X[self.key]\n","    \n","class NumberSelector(BaseEstimator, TransformerMixin):\n","    \"\"\"\n","    Transformer to select a single column from the data frame to perform additional transformations on\n","    Use on numeric columns in the data\n","    \"\"\"\n","    def __init__(self, key):\n","        self.key = key\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return X[[self.key]]\n","    \n"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b6a864df-d1f5-4199-aba6-658c6cce2c49","_uuid":"f0cbfcf81d77373d88bd24522020c8469abeb9a0"},"source":["Here a pipeline is formed that will retrieve the `processed` text column from a\n","dataframe, then applies the `TfidfVectorizer` to the resulting Pandas Series.\n","\n","I have explained the `TfidfVectorizer` in detail in a previous assignment."]},{"cell_type":"code","execution_count":157,"metadata":{"_cell_guid":"55de63e6-8d2f-478f-9126-6e7b0546207c","_uuid":"2caac353c114eb6cc7e493eecfa90639cadb5e84"},"outputs":[{"data":{"text/plain":["<13117x21516 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 148061 stored elements in Compressed Sparse Row format>"]},"execution_count":157,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","text = Pipeline([\n","                ('selector', TextSelector(key='processed')),\n","                ('tfidf', TfidfVectorizer( stop_words='english'))\n","            ])\n","\n","text.fit_transform(X_train)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"dec38474-3a0a-45d3-a52f-f99dd72f0f2a","_uuid":"17e07518d728d90fe52483eeec70b7bdda9b8c12"},"source":["A similar pipeline is applied to the numeric data from the dataframe.  In this\n","case the data is standardized by removing the mean and scaling to unit variance.\n","This score is calculated by applying the following function on each sample, $x$:\n","$$ z = \\frac{x - \\mu}{\\sigma}. $$\n","\n","This is done to ensure that the data is centered around $0$ and looks similar to\n","a standard normal distribution because many parts used in the objective function\n","of learning algorithms assume standard distribution."]},{"cell_type":"code","execution_count":158,"metadata":{"_cell_guid":"ea77456b-f7cc-4480-97a5-4b4c819de7a1","_uuid":"eb59242f760520108b9b8f73c108f584af3130f0"},"outputs":[{"data":{"text/plain":["array([[-0.50769254],\n","       [ 0.88000324],\n","       [ 2.24907223],\n","       ...,\n","       [-0.46112557],\n","       [-0.14447015],\n","       [-0.39593181]])"]},"execution_count":158,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import StandardScaler\n","\n","length =  Pipeline([\n","                ('selector', NumberSelector(key='length')),\n","                ('standard', StandardScaler())\n","            ])\n","\n","length.fit_transform(X_train)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"71f2c594-9bd8-4d4e-bd3c-7316a604bcdf","_uuid":"ec08bbc51a6226c4d725936b36243b819bd43d1a"},"source":["The same process is followed for each of the other numeric features."]},{"cell_type":"code","execution_count":159,"metadata":{"_cell_guid":"80d80de4-bede-4cdf-8047-e3bb878c6609","_uuid":"93fde50658fe3a3ce9b481d66f8ff39570a6be10","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["words =  Pipeline([\n","                ('selector', NumberSelector(key='words')),\n","                ('standard', StandardScaler())\n","            ])\n","words_not_stopword =  Pipeline([\n","                ('selector', NumberSelector(key='words_not_stopword')),\n","                ('standard', StandardScaler())\n","            ])\n","avg_word_length =  Pipeline([\n","                ('selector', NumberSelector(key='avg_word_length')),\n","                ('standard', StandardScaler())\n","            ])\n","commas =  Pipeline([\n","                ('selector', NumberSelector(key='commas')),\n","                ('standard', StandardScaler()),\n","            ])\n","adj_count =  Pipeline([\n","                ('selector', NumberSelector(key='adj_count')),\n","                ('standard', StandardScaler())\n","            ])\n","noun_count =  Pipeline([\n","                ('selector', NumberSelector(key='noun_count')),\n","                ('standard', StandardScaler())\n","            ])\n","verb_count =  Pipeline([\n","                ('selector', NumberSelector(key='verb_count')),\n","                ('standard', StandardScaler()),\n","            ])"]},{"cell_type":"markdown","metadata":{"_cell_guid":"150d3ce6-9fed-4ce0-a3e9-8f955ad7874d","_uuid":"2a709500025294ef8a4cfda02f7a7d02a96ff61b"},"source":["`FeatureUnion` is then used to create a concatenated results from all the\n","`Pipeline` objects.  This resulting estimator applies the pipelines in parallel.\n","The author then puts the FeatureUnion estimator into a pipeline to demonstrate a\n","fit/transformation on the training set."]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[],"source":["from sklearn.pipeline import FeatureUnion\n","\n","feats_original = FeatureUnion([('text', text), \n","                      ('length', length),\n","                      ('words', words),\n","                      ('words_not_stopword', words_not_stopword),\n","                      ('avg_word_length', avg_word_length),\n","                      ('commas', commas)])"]},{"cell_type":"markdown","metadata":{},"source":["Here, I added the three new features to the `FeatureUnion` for analysis."]},{"cell_type":"code","execution_count":161,"metadata":{"_cell_guid":"95d16585-367f-4e74-98fd-bd266e19a672","_uuid":"65f49fd19c57ba619518b2a0de615e08cb4f3c5f"},"outputs":[],"source":["feats = FeatureUnion([('text', text), \n","                      ('length', length),\n","                      ('words', words),\n","                      ('words_not_stopword', words_not_stopword),\n","                      ('avg_word_length', avg_word_length),\n","                      ('commas', commas),\n","                      ('adj_count', adj_count),\n","                      ('noun_count', noun_count),\n","                      ('verb_count', verb_count)])"]},{"cell_type":"markdown","metadata":{},"source":["### Adding a Classifier\n","\n","Ultimately, the author expanded the previous cell by adding a `RandomForestClassifier`\n","to the pipeline so that it can be trained/fit to the dataset.  Notice that the\n","model predicted the test set correctly $67.92\\%$ of the time.  In our case, we\n","will be using Logistic Regression instead as shown in the cell following this\n","commented out cell."]},{"cell_type":"code","execution_count":30,"metadata":{"_cell_guid":"84b4ce5d-c4a3-46f7-903f-c4c2d7518ecd","_uuid":"acaeeee80c58268f3b21d9d2c1cd7a9a3b5fcd21"},"outputs":[{"data":{"text/plain":["0.6792014856081708"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# from sklearn.ensemble import RandomForestClassifier\n","\n","# pipeline = Pipeline([\n","#     ('features',feats),\n","#     ('classifier', RandomForestClassifier(random_state = 42)),\n","# ])\n","\n","# pipeline.fit(X_train, y_train)\n","\n","# preds = pipeline.predict(X_test)\n","# np.mean(preds == y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### 2a Using Logistic Regression instead of Random Forest\n","\n","This cell implements the use of Logistic Regression over Random Forest"]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original:  0.7807180439492417\n","New:  0.780099040544723\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","\n","pipeline_original = Pipeline([\n","    ('features',feats_original),\n","    ('classifier', LogisticRegression(max_iter=350, random_state = 42)),\n","])\n","\n","pipeline = Pipeline([\n","    ('features',feats),\n","    ('classifier', LogisticRegression(max_iter=350, random_state = 42)),\n","])\n","\n","pipeline_original.fit(X_train, y_train)\n","pipeline.fit(X_train, y_train)\n","\n","preds_original = pipeline_original.predict(X_test)\n","preds = pipeline.predict(X_test)\n","print(\"Original: \", np.mean(preds_original == y_test))\n","print(\"New: \", np.mean(preds == y_test))"]},{"cell_type":"markdown","metadata":{},"source":["### 2c - Classification Report\n","\n","This is the classification report before cross-validation/hyperparameter tuning.\n","Although, it is not shown here, Logistic Regression seems to perform\n","significantly better than Random Forest."]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Old model:\n","              precision    recall  f1-score   support\n","\n","         EAP       0.74      0.85      0.79      2587\n","         HPL       0.81      0.74      0.77      1852\n","         MWS       0.83      0.73      0.78      2023\n","\n","    accuracy                           0.78      6462\n","   macro avg       0.79      0.77      0.78      6462\n","weighted avg       0.79      0.78      0.78      6462\n","\n","New model (with 3 added features):\n","              precision    recall  f1-score   support\n","\n","         EAP       0.74      0.84      0.79      2587\n","         HPL       0.81      0.75      0.78      1852\n","         MWS       0.81      0.73      0.77      2023\n","\n","    accuracy                           0.78      6462\n","   macro avg       0.79      0.77      0.78      6462\n","weighted avg       0.78      0.78      0.78      6462\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","print(\"Old model:\")\n","print(classification_report(y_test, preds_original))\n","\n","print(\"New model (with 3 added features):\")\n","print(classification_report(y_test, preds))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"10444a55-f32b-41cd-8dbd-c1ea63b4c9cb","_uuid":"66c104071c5e7bab167da53c4747610f6e568a39"},"source":["## Cross Validation\n","\n","As was covered in class, we want to tune the parameters.  Here, the original\n","author explores the parameters available for tuning."]},{"cell_type":"code","execution_count":135,"metadata":{"_cell_guid":"6ce23a11-a9e8-455e-857e-249615b89066","_uuid":"c0bfe59aaf80d0aacf57eced7fad9252a24a6a4c"},"outputs":[{"data":{"text/plain":["dict_keys(['memory', 'steps', 'verbose', 'features', 'classifier', 'features__n_jobs', 'features__transformer_list', 'features__transformer_weights', 'features__verbose', 'features__text', 'features__length', 'features__words', 'features__words_not_stopword', 'features__avg_word_length', 'features__commas', 'features__adj_count', 'features__noun_count', 'features__verb_count', 'features__text__memory', 'features__text__steps', 'features__text__verbose', 'features__text__selector', 'features__text__tfidf', 'features__text__selector__key', 'features__text__tfidf__analyzer', 'features__text__tfidf__binary', 'features__text__tfidf__decode_error', 'features__text__tfidf__dtype', 'features__text__tfidf__encoding', 'features__text__tfidf__input', 'features__text__tfidf__lowercase', 'features__text__tfidf__max_df', 'features__text__tfidf__max_features', 'features__text__tfidf__min_df', 'features__text__tfidf__ngram_range', 'features__text__tfidf__norm', 'features__text__tfidf__preprocessor', 'features__text__tfidf__smooth_idf', 'features__text__tfidf__stop_words', 'features__text__tfidf__strip_accents', 'features__text__tfidf__sublinear_tf', 'features__text__tfidf__token_pattern', 'features__text__tfidf__tokenizer', 'features__text__tfidf__use_idf', 'features__text__tfidf__vocabulary', 'features__length__memory', 'features__length__steps', 'features__length__verbose', 'features__length__selector', 'features__length__standard', 'features__length__selector__key', 'features__length__standard__copy', 'features__length__standard__with_mean', 'features__length__standard__with_std', 'features__words__memory', 'features__words__steps', 'features__words__verbose', 'features__words__selector', 'features__words__standard', 'features__words__selector__key', 'features__words__standard__copy', 'features__words__standard__with_mean', 'features__words__standard__with_std', 'features__words_not_stopword__memory', 'features__words_not_stopword__steps', 'features__words_not_stopword__verbose', 'features__words_not_stopword__selector', 'features__words_not_stopword__standard', 'features__words_not_stopword__selector__key', 'features__words_not_stopword__standard__copy', 'features__words_not_stopword__standard__with_mean', 'features__words_not_stopword__standard__with_std', 'features__avg_word_length__memory', 'features__avg_word_length__steps', 'features__avg_word_length__verbose', 'features__avg_word_length__selector', 'features__avg_word_length__standard', 'features__avg_word_length__selector__key', 'features__avg_word_length__standard__copy', 'features__avg_word_length__standard__with_mean', 'features__avg_word_length__standard__with_std', 'features__commas__memory', 'features__commas__steps', 'features__commas__verbose', 'features__commas__selector', 'features__commas__standard', 'features__commas__selector__key', 'features__commas__standard__copy', 'features__commas__standard__with_mean', 'features__commas__standard__with_std', 'features__adj_count__memory', 'features__adj_count__steps', 'features__adj_count__verbose', 'features__adj_count__selector', 'features__adj_count__standard', 'features__adj_count__selector__key', 'features__adj_count__standard__copy', 'features__adj_count__standard__with_mean', 'features__adj_count__standard__with_std', 'features__noun_count__memory', 'features__noun_count__steps', 'features__noun_count__verbose', 'features__noun_count__selector', 'features__noun_count__standard', 'features__noun_count__selector__key', 'features__noun_count__standard__copy', 'features__noun_count__standard__with_mean', 'features__noun_count__standard__with_std', 'features__verb_count__memory', 'features__verb_count__steps', 'features__verb_count__verbose', 'features__verb_count__selector', 'features__verb_count__standard', 'features__verb_count__selector__key', 'features__verb_count__standard__copy', 'features__verb_count__standard__with_mean', 'features__verb_count__standard__with_std', 'classifier__C', 'classifier__class_weight', 'classifier__dual', 'classifier__fit_intercept', 'classifier__intercept_scaling', 'classifier__l1_ratio', 'classifier__max_iter', 'classifier__multi_class', 'classifier__n_jobs', 'classifier__penalty', 'classifier__random_state', 'classifier__solver', 'classifier__tol', 'classifier__verbose', 'classifier__warm_start'])"]},"execution_count":135,"metadata":{},"output_type":"execute_result"}],"source":["pipeline.get_params().keys()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"dbfb2342-2817-4ced-a609-9938562e01c5","_uuid":"309d37e42e11abfdddc127acebe8d2bcb2b452df"},"source":["The original author chose the hyperparameters as shown in the cell below, then\n","does a `GridSearchCV` to tune with the list of values in the hyperparameter dictionary."]},{"cell_type":"code","execution_count":147,"metadata":{"_cell_guid":"602a1465-ff99-44f2-9d4d-4b6847afba2e","_uuid":"7d2b420ef99bf024a2bd057f97538ab471674fd9"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n","             estimator=Pipeline(steps=[(&#x27;features&#x27;,\n","                                        FeatureUnion(transformer_list=[(&#x27;text&#x27;,\n","                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                                         TextSelector(key=&#x27;processed&#x27;)),\n","                                                                                        (&#x27;tfidf&#x27;,\n","                                                                                         TfidfVectorizer(stop_words=&#x27;english&#x27;))])),\n","                                                                       (&#x27;length&#x27;,\n","                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                                         NumberSelector(key=&#x27;length&#x27;)),\n","                                                                                        (&#x27;standard&#x27;,\n","                                                                                         StandardScaler())])),\n","                                                                       (&#x27;words&#x27;,\n","                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                                         NumberS...\n","                                                                       (&#x27;verb_count&#x27;,\n","                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                                         NumberSelector(key=&#x27;verb_count&#x27;)),\n","                                                                                        (&#x27;standard&#x27;,\n","                                                                                         StandardScaler())]))])),\n","                                       (&#x27;classifier&#x27;,\n","                                        LogisticRegression(max_iter=400,\n","                                                           random_state=42))]),\n","             param_grid={&#x27;classifier__C&#x27;: [0.95, 1.05],\n","                         &#x27;classifier__intercept_scaling&#x27;: [0.95, 1.05],\n","                         &#x27;features__text__tfidf__max_df&#x27;: [0.9, 0.95],\n","                         &#x27;features__text__tfidf__ngram_range&#x27;: [(1, 1),\n","                                                                (1, 2)]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n","             estimator=Pipeline(steps=[(&#x27;features&#x27;,\n","                                        FeatureUnion(transformer_list=[(&#x27;text&#x27;,\n","                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                                         TextSelector(key=&#x27;processed&#x27;)),\n","                                                                                        (&#x27;tfidf&#x27;,\n","                                                                                         TfidfVectorizer(stop_words=&#x27;english&#x27;))])),\n","                                                                       (&#x27;length&#x27;,\n","                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                                         NumberSelector(key=&#x27;length&#x27;)),\n","                                                                                        (&#x27;standard&#x27;,\n","                                                                                         StandardScaler())])),\n","                                                                       (&#x27;words&#x27;,\n","                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                                         NumberS...\n","                                                                       (&#x27;verb_count&#x27;,\n","                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                                         NumberSelector(key=&#x27;verb_count&#x27;)),\n","                                                                                        (&#x27;standard&#x27;,\n","                                                                                         StandardScaler())]))])),\n","                                       (&#x27;classifier&#x27;,\n","                                        LogisticRegression(max_iter=400,\n","                                                           random_state=42))]),\n","             param_grid={&#x27;classifier__C&#x27;: [0.95, 1.05],\n","                         &#x27;classifier__intercept_scaling&#x27;: [0.95, 1.05],\n","                         &#x27;features__text__tfidf__max_df&#x27;: [0.9, 0.95],\n","                         &#x27;features__text__tfidf__ngram_range&#x27;: [(1, 1),\n","                                                                (1, 2)]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n","                 FeatureUnion(transformer_list=[(&#x27;text&#x27;,\n","                                                 Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                  TextSelector(key=&#x27;processed&#x27;)),\n","                                                                 (&#x27;tfidf&#x27;,\n","                                                                  TfidfVectorizer(stop_words=&#x27;english&#x27;))])),\n","                                                (&#x27;length&#x27;,\n","                                                 Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                  NumberSelector(key=&#x27;length&#x27;)),\n","                                                                 (&#x27;standard&#x27;,\n","                                                                  StandardScaler())])),\n","                                                (&#x27;words&#x27;,\n","                                                 Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                  NumberSelector(key=&#x27;words&#x27;)),\n","                                                                 (&#x27;stan...\n","                                                                  NumberSelector(key=&#x27;adj_count&#x27;)),\n","                                                                 (&#x27;standard&#x27;,\n","                                                                  StandardScaler())])),\n","                                                (&#x27;noun_count&#x27;,\n","                                                 Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                  NumberSelector(key=&#x27;noun_count&#x27;)),\n","                                                                 (&#x27;standard&#x27;,\n","                                                                  StandardScaler())])),\n","                                                (&#x27;verb_count&#x27;,\n","                                                 Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                                  NumberSelector(key=&#x27;verb_count&#x27;)),\n","                                                                 (&#x27;standard&#x27;,\n","                                                                  StandardScaler())]))])),\n","                (&#x27;classifier&#x27;,\n","                 LogisticRegression(max_iter=400, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">features: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;text&#x27;,\n","                                Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                 TextSelector(key=&#x27;processed&#x27;)),\n","                                                (&#x27;tfidf&#x27;,\n","                                                 TfidfVectorizer(stop_words=&#x27;english&#x27;))])),\n","                               (&#x27;length&#x27;,\n","                                Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                 NumberSelector(key=&#x27;length&#x27;)),\n","                                                (&#x27;standard&#x27;,\n","                                                 StandardScaler())])),\n","                               (&#x27;words&#x27;,\n","                                Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                 NumberSelector(key=&#x27;words&#x27;)),\n","                                                (&#x27;standard&#x27;,\n","                                                 StandardScaler())])),\n","                               (...\n","                                                (&#x27;standard&#x27;,\n","                                                 StandardScaler())])),\n","                               (&#x27;adj_count&#x27;,\n","                                Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                 NumberSelector(key=&#x27;adj_count&#x27;)),\n","                                                (&#x27;standard&#x27;,\n","                                                 StandardScaler())])),\n","                               (&#x27;noun_count&#x27;,\n","                                Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                 NumberSelector(key=&#x27;noun_count&#x27;)),\n","                                                (&#x27;standard&#x27;,\n","                                                 StandardScaler())])),\n","                               (&#x27;verb_count&#x27;,\n","                                Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                 NumberSelector(key=&#x27;verb_count&#x27;)),\n","                                                (&#x27;standard&#x27;,\n","                                                 StandardScaler())]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>text</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TextSelector</label><div class=\"sk-toggleable__content\"><pre>TextSelector(key=&#x27;processed&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>length</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;length&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>words</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\" ><label for=\"sk-estimator-id-47\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;words&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-48\" type=\"checkbox\" ><label for=\"sk-estimator-id-48\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>words_not_stopword</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-49\" type=\"checkbox\" ><label for=\"sk-estimator-id-49\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;words_not_stopword&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-50\" type=\"checkbox\" ><label for=\"sk-estimator-id-50\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>avg_word_length</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\" ><label for=\"sk-estimator-id-51\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;avg_word_length&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\" ><label for=\"sk-estimator-id-52\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>commas</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" ><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;commas&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-54\" type=\"checkbox\" ><label for=\"sk-estimator-id-54\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>adj_count</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" ><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;adj_count&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\" ><label for=\"sk-estimator-id-56\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>noun_count</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-57\" type=\"checkbox\" ><label for=\"sk-estimator-id-57\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;noun_count&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-58\" type=\"checkbox\" ><label for=\"sk-estimator-id-58\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>verb_count</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-59\" type=\"checkbox\" ><label for=\"sk-estimator-id-59\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;verb_count&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-60\" type=\"checkbox\" ><label for=\"sk-estimator-id-60\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-61\" type=\"checkbox\" ><label for=\"sk-estimator-id-61\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=400, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"],"text/plain":["GridSearchCV(cv=5,\n","             estimator=Pipeline(steps=[('features',\n","                                        FeatureUnion(transformer_list=[('text',\n","                                                                        Pipeline(steps=[('selector',\n","                                                                                         TextSelector(key='processed')),\n","                                                                                        ('tfidf',\n","                                                                                         TfidfVectorizer(stop_words='english'))])),\n","                                                                       ('length',\n","                                                                        Pipeline(steps=[('selector',\n","                                                                                         NumberSelector(key='length')),\n","                                                                                        ('standard',\n","                                                                                         StandardScaler())])),\n","                                                                       ('words',\n","                                                                        Pipeline(steps=[('selector',\n","                                                                                         NumberS...\n","                                                                       ('verb_count',\n","                                                                        Pipeline(steps=[('selector',\n","                                                                                         NumberSelector(key='verb_count')),\n","                                                                                        ('standard',\n","                                                                                         StandardScaler())]))])),\n","                                       ('classifier',\n","                                        LogisticRegression(max_iter=400,\n","                                                           random_state=42))]),\n","             param_grid={'classifier__C': [0.95, 1.05],\n","                         'classifier__intercept_scaling': [0.95, 1.05],\n","                         'features__text__tfidf__max_df': [0.9, 0.95],\n","                         'features__text__tfidf__ngram_range': [(1, 1),\n","                                                                (1, 2)]})"]},"execution_count":147,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import GridSearchCV\n","\n","hyperparameters = { 'features__text__tfidf__max_df': [0.8, 0.95],\n","                    'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n","                    'classifier__C': [0.9, 1.1],\n","                    'classifier__intercept_scaling': [0.9, 1.1]\n","                  }\n","clf_old = GridSearchCV(pipeline_original, hyperparameters, cv=5)\n","clf = GridSearchCV(pipeline, hyperparameters, cv=5)\n"," \n","# Fit and tune the old model\n","clf_old.fit(X_train, y_train)\n"," \n","# Fit and tune the new model\n","clf.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"16c7c9d1-4c17-4619-a2f1-138de837e1cf","_uuid":"9ad824ce203554cdda13502640f80e8d2c4ddc9d"},"source":["The best parameters are found using the `best_params_` variable."]},{"cell_type":"code","execution_count":148,"metadata":{"_cell_guid":"d1ee7fb2-c946-41b9-adbb-3ac449b04f7e","_uuid":"bc5e2c7551604acbb3deb49fa662f21710268bf7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Old Model Best Params:\n","{'classifier__C': 1.05, 'classifier__intercept_scaling': 0.95, 'features__text__tfidf__max_df': 0.9, 'features__text__tfidf__ngram_range': (1, 1)}\n","New Model Best Params:\n","{'classifier__C': 1.05, 'classifier__intercept_scaling': 0.95, 'features__text__tfidf__max_df': 0.9, 'features__text__tfidf__ngram_range': (1, 1)}\n"]}],"source":["print(\"Old Model Best Params:\")\n","print(clf_old.best_params_)\n","\n","print(\"New Model Best Params:\")\n","print(clf.best_params_)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0b2899ab-ca1e-4814-a81c-258aa28224cf","_uuid":"db2f77237f9026de72b29384a870a7d1adab5cf3"},"source":["Finally, we can refit the model to the best parameters using `refit`."]},{"cell_type":"code","execution_count":149,"metadata":{"_cell_guid":"23ed5f54-0d58-4a2d-ad7a-ca8d80f5574c","_uuid":"d9eba0d0ee5dce9f777c633881bcdd19eda6c9c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Old model:\n","0.7816465490560198\n","New Model:\n","0.7807180439492417\n"]}],"source":["#refitting on entire training data using best settings\n","clf_old.refit\n","clf.refit\n","\n","preds_old = clf_old.predict(X_test)\n","probs_old = clf_old.predict_proba(X_test)\n","preds = clf.predict(X_test)\n","probs = clf.predict_proba(X_test)\n","\n","print(\"Old model:\")\n","print(np.mean(preds_old == y_test))\n","print(\"New Model:\")\n","print(np.mean(preds == y_test))"]},{"cell_type":"markdown","metadata":{},"source":["### 2c - Classification Report\n","\n","This is the classification report after cross-validation/hyperparameter tuning."]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Old model:\n","              precision    recall  f1-score   support\n","\n","         EAP       0.74      0.85      0.79      2587\n","         HPL       0.81      0.74      0.77      1852\n","         MWS       0.83      0.73      0.78      2023\n","\n","    accuracy                           0.78      6462\n","   macro avg       0.79      0.77      0.78      6462\n","weighted avg       0.79      0.78      0.78      6462\n","\n","New model (with 3 added features):\n","              precision    recall  f1-score   support\n","\n","         EAP       0.74      0.84      0.79      2587\n","         HPL       0.81      0.75      0.78      1852\n","         MWS       0.82      0.73      0.77      2023\n","\n","    accuracy                           0.78      6462\n","   macro avg       0.79      0.77      0.78      6462\n","weighted avg       0.78      0.78      0.78      6462\n","\n"]}],"source":["print(\"Old model:\")\n","print(classification_report(y_test, preds_original))\n","\n","print(\"New model (with 3 added features):\")\n","print(classification_report(y_test, preds))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"2f6094d1-2f14-4188-930a-d67200f49738","_uuid":"f3e4177bc191ee0d529bf84ef60fe89c34b4f6bd"},"source":["# Final Predictions\n","\n","Here, the final results are gathered with the probability score for each author\n","for each sentence."]},{"cell_type":"code","execution_count":152,"metadata":{"_cell_guid":"7e8526e3-f855-4ace-b6fb-2e00dbdf8d0b","_uuid":"1b7038b1d5a30479213799839531867afa91da5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Old:\n","              EAP       HPL       MWS\n","id                                   \n","id02310  0.292199  0.062944  0.644857\n","id24541  0.848385  0.047281  0.104334\n","id00134  0.238463  0.685973  0.075564\n","id27757  0.682623  0.211395  0.105982\n","id04081  0.712984  0.211258  0.075758\n","\n","New:\n","              EAP       HPL       MWS\n","id                                   \n","id02310  0.236220  0.063602  0.700179\n","id24541  0.835888  0.037979  0.126133\n","id00134  0.243195  0.694784  0.062021\n","id27757  0.702322  0.192928  0.104749\n","id04081  0.694676  0.218217  0.087106\n"]}],"source":["submission = pd.read_csv('./input/test.csv')\n","\n","#preprocessing\n","submission = processing(submission)\n","predictions_old = clf_old.predict_proba(submission)\n","predictions = clf.predict_proba(submission)\n","\n","preds_old = pd.DataFrame(data=predictions_old, columns = clf_old.best_estimator_.named_steps['classifier'].classes_)\n","preds = pd.DataFrame(data=predictions, columns = clf.best_estimator_.named_steps['classifier'].classes_)\n","\n","#generating a submission file\n","result_old = pd.concat([submission[['id']], preds_old], axis=1)\n","result_old.set_index('id', inplace = True)\n","print(\"Old:\")\n","print(result_old.head())\n","print()\n","\n","result = pd.concat([submission[['id']], preds], axis=1)\n","result.set_index('id', inplace = True)\n","print(\"New:\")\n","print(result.head())"]},{"cell_type":"markdown","metadata":{},"source":["# Discussion\n","\n","Looking at the classification report, we can see that the new model performed\n","slightly worse than the old model.  The reason for this is unclear since I would\n","have thought that the addition of the new features\n","would have increased the precision and recall.\n","\n","One item of note is that the model\n","performance between the old model and the new model is pretty small.  Without\n","some statistical analysis, we don't really know if the difference is\n","statistically significant.  Given this knowledge, we really cannot conclude\n","anything and any analysis of this data without a deeper statistical analysis\n","would be anecdotal at best.\n","\n","Yet another dimension to all of this is the differences in performance from the\n","initial fit and the model with tuned hyperparameters.  It seems that the\n","hyperparameter tuning I chose did not make much of a difference to the final\n","results.  All of this requires further investigation and analysis."]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 ('csc620')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"a71d39d660cde086a1906713d7782789105334eb77ec37352255bdee7f037f90"}}},"nbformat":4,"nbformat_minor":4}
