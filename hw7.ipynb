<<<<<<< HEAD
{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/Caskroom/miniconda/base/envs/csc620/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n","  from pandas import MultiIndex, Int64Index\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","import re\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from nltk.tokenize import TweetTokenizer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import confusion_matrix\n","import xgboost\n","from sklearn.model_selection import RandomizedSearchCV\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["train=pd.read_csv(\"./Corona_NLP_train.csv\",encoding='latin1')"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>UserName</th>\n","      <th>ScreenName</th>\n","      <th>Location</th>\n","      <th>TweetAt</th>\n","      <th>OriginalTweet</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3799</td>\n","      <td>48751</td>\n","      <td>London</td>\n","      <td>16-03-2020</td>\n","      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3800</td>\n","      <td>48752</td>\n","      <td>UK</td>\n","      <td>16-03-2020</td>\n","      <td>advice Talk to your neighbours family to excha...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3801</td>\n","      <td>48753</td>\n","      <td>Vagabonds</td>\n","      <td>16-03-2020</td>\n","      <td>Coronavirus Australia: Woolworths to give elde...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3802</td>\n","      <td>48754</td>\n","      <td>NaN</td>\n","      <td>16-03-2020</td>\n","      <td>My food stock is not the only one which is emp...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3803</td>\n","      <td>48755</td>\n","      <td>NaN</td>\n","      <td>16-03-2020</td>\n","      <td>Me, ready to go at supermarket during the #COV...</td>\n","      <td>Extremely Negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   UserName  ScreenName   Location     TweetAt  \\\n","0      3799       48751     London  16-03-2020   \n","1      3800       48752         UK  16-03-2020   \n","2      3801       48753  Vagabonds  16-03-2020   \n","3      3802       48754        NaN  16-03-2020   \n","4      3803       48755        NaN  16-03-2020   \n","\n","                                       OriginalTweet           Sentiment  \n","0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n","1  advice Talk to your neighbours family to excha...            Positive  \n","2  Coronavirus Australia: Woolworths to give elde...            Positive  \n","3  My food stock is not the only one which is emp...            Positive  \n","4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["def drop(p):\n","    p.drop([\"UserName\",\"ScreenName\",\"Location\",\"TweetAt\"],axis=1,inplace=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["drop(train)"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>OriginalTweet</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>advice Talk to your neighbours family to excha...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Coronavirus Australia: Woolworths to give elde...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>My food stock is not the only one which is emp...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Me, ready to go at supermarket during the #COV...</td>\n","      <td>Extremely Negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       OriginalTweet           Sentiment\n","0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n","1  advice Talk to your neighbours family to excha...            Positive\n","2  Coronavirus Australia: Woolworths to give elde...            Positive\n","3  My food stock is not the only one which is emp...            Positive\n","4  Me, ready to go at supermarket during the #COV...  Extremely Negative"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["Positive              11422\n","Negative               9917\n","Neutral                7713\n","Extremely Positive     6624\n","Extremely Negative     5481\n","Name: Sentiment, dtype: int64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train[\"Sentiment\"].value_counts()"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["def rep(t):\n","        d={\"Sentiment\":{'Positive':0,'Negative':1,\"Neutral\":2,\"Extremely Positive\":3,\"Extremely Negative\":4}}\n","        t.replace(d,inplace=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["rep(train)"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>OriginalTweet</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>advice Talk to your neighbours family to excha...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Coronavirus Australia: Woolworths to give elde...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>My food stock is not the only one which is emp...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Me, ready to go at supermarket during the #COV...</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       OriginalTweet  Sentiment\n","0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...          2\n","1  advice Talk to your neighbours family to excha...          0\n","2  Coronavirus Australia: Woolworths to give elde...          0\n","3  My food stock is not the only one which is emp...          0\n","4  Me, ready to go at supermarket during the #COV...          4"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["tweettoken = TweetTokenizer(strip_handles=True, reduce_len=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["lemmatizer=WordNetLemmatizer()"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["stemmer=PorterStemmer()"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["collect=[]\n","def preprocess(t):\n","    tee=re.sub('[^a-zA-Z]',\" \",t)\n","    tee=tee.lower()\n","    res=tweettoken.tokenize(tee)\n","    for i in res:\n","        if i in stopwords.words('english'):\n","            res.remove(i)\n","    rest=[]\n","    for k in res:\n","        rest.append(lemmatizer.lemmatize(k))\n","    ret=\" \".join(rest)\n","    collect.append(ret)\n","    "]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["for j in range(41157):\n","    preprocess(train[\"OriginalTweet\"].iloc[j])"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["['menyrbie phil gahan chrisitv http co ifz fan pa http co xx ghgfzcc http co nlzdxno',\n"," 'advice talk your neighbour family exchange phone number create contact list phone number neighbour school employer chemist gp set online shopping account po adequate supply regular med not order',\n"," 'coronavirus australia woolworth give elderly disabled dedicated shopping hour amid covid outbreak http co binca vp p',\n"," 'food stock not only one is empty please panic will enough food everyone not take than you need stay calm stay safe covid france covid covid coronavirus confinement confinementotal confinementgeneral http t co zrlg z j',\n"," 'ready go supermarket covid outbreak because m paranoid because food stock litteraly empty the coronavirus a serious thing please panic cause shortage coronavirusfrance restezchezvous stayathome confinement http t co usmualq n']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["collect[:5]"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":["def bow(ll):\n","    cv=CountVectorizer(max_features=200)\n","    x=cv.fit_transform(ll).toarray()\n","    return x\n","    "]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[],"source":["y=bow(collect)"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0]])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["y[:1]"]},{"cell_type":"code","execution_count":29,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["200"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["len(y[0][:])"]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":true},"outputs":[],"source":["def tfidf(xx):\n","    cv=TfidfVectorizer(max_features=4000)\n","    x=cv.fit_transform(xx).toarray()\n","    return x"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[],"source":["values=train[\"Sentiment\"].values"]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([2, 0, 0, ..., 0, 2, 1])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["values"]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[],"source":["(x_train,x_test,y_train,y_test) = train_test_split(y,values, train_size=0.75, random_state=42)"]},{"cell_type":"code","execution_count":34,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([[0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       ...,\n","       [0, 0, 0, ..., 0, 1, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0]])"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["x_train"]},{"cell_type":"code","execution_count":35,"metadata":{"trusted":true},"outputs":[],"source":["rnd_clf=RandomForestClassifier(n_estimators=200,random_state=42)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=200, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=200, random_state=42)</pre></div></div></div></div></div>"],"text/plain":["RandomForestClassifier(n_estimators=200, random_state=42)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["rnd_clf.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":37,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0.4172983479105928"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["rnd_clf.score(x_test,y_test)"]},{"cell_type":"code","execution_count":38,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([[1358,  592,  501,  338,  109],\n","       [ 702, 1014,  448,  106,  243],\n","       [ 477,  438,  913,   46,   45],\n","       [ 653,  183,  162,  617,   28],\n","       [ 287,  457,  148,   33,  392]])"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["y_pred=rnd_clf.predict(x_test)\n","cm=confusion_matrix(y_test,y_pred)\n","cm"]},{"cell_type":"code","execution_count":39,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.4152575315840622\n","0.41438289601554906\n","0.41438289601554906\n","0.41243926141885323\n","0.41564625850340137\n","0.4182701652089407\n","0.41778425655976675\n"]}],"source":["a=[400,500,600,700,800,900,1000]\n","for i in a:\n","    rnd_clf=RandomForestClassifier(n_estimators=i,random_state=42)\n","    rnd_clf.fit(x_train,y_train)\n","    t=rnd_clf.score(x_test,y_test)\n","    print(t)"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'x_train' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnaive_bayes\u001b[39;00m \u001b[39mimport\u001b[39;00m MultinomialNB\n\u001b[1;32m      2\u001b[0m clf \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[0;32m----> 3\u001b[0m clf\u001b[39m.\u001b[39mfit(x_train,y_train)\n","\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"]}],"source":["from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB()\n","clf.fit(x_train,y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clf.score(x_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y=tfidf(collect)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["(x_train,x_test,y_train,y_test) = train_test_split(y,values, train_size=0.75, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rnd_clf=RandomForestClassifier(n_estimators=200,max_leaf_nodes=8,random_state=42)\n","rnd_clf.fit(x_train,y_train)\n","rnd_clf.score(x_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clf = MultinomialNB()\n","clf.fit(x_train,y_train)\n","clf.score(x_test,y_test)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 ('csc620')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"a71d39d660cde086a1906713d7782789105334eb77ec37352255bdee7f037f90"}}},"nbformat":4,"nbformat_minor":4}
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC 620 - HA #7 - Text Classification using Naive Bayes\n",
    "\n",
    "Mark Kim\n",
    "\n",
    "Implementation taken from: [Jayant\n",
    "Awasthi](https://www.kaggle.com/code/jayantawasthi/nlp-corona-tweet-with-random-forest-and-naivebayes/notebook)\n",
    "\n",
    "This notebook implements sentiment text classification from twitter posts about\n",
    "the Coronavirus.  The data was manually tagged as: Extremely Negative, Negative, Neutral,\n",
    "Positive, or Extremely Positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "These are the package imports used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test.csv\n",
      "./csc620env.yml\n",
      "./hw7.ipynb\n",
      "./.conda_config\n",
      "./test_predictions.csv\n",
      "./hw5.ipynb\n",
      "./.gitignore\n",
      "./model.csv\n",
      "./Corona_NLP_train.csv\n",
      "./eliza_hw2.ipynb\n",
      "./Corona_NLP_test.csv\n",
      "./hw3.ipynb\n",
      "./train.csv\n",
      "./hw4.ipynb\n",
      "./hw6.ipynb\n",
      "./eliza.py\n",
      "./file_archive/trumptweets.csv\n",
      "./file_archive/Screen Shot 2022-08-31 at 4.30.19 PM.png\n",
      "./file_archive/realdonaldtrump.csv\n",
      "./.ipynb_checkpoints/eliza_hw2-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/hw3-checkpoint.ipynb\n",
      "./.git/config\n",
      "./.git/HEAD\n",
      "./.git/description\n",
      "./.git/index\n",
      "./.git/packed-refs\n",
      "./.git/objects/pack/pack-df2b6aeaff82858ec31e9a0ee48db39cf8ee3fb7.pack\n",
      "./.git/objects/pack/pack-df2b6aeaff82858ec31e9a0ee48db39cf8ee3fb7.idx\n",
      "./.git/info/exclude\n",
      "./.git/logs/HEAD\n",
      "./.git/logs/refs/heads/hw7\n",
      "./.git/logs/refs/heads/ha1\n",
      "./.git/logs/refs/remotes/origin/HEAD\n",
      "./.git/hooks/commit-msg.sample\n",
      "./.git/hooks/pre-rebase.sample\n",
      "./.git/hooks/pre-commit.sample\n",
      "./.git/hooks/applypatch-msg.sample\n",
      "./.git/hooks/fsmonitor-watchman.sample\n",
      "./.git/hooks/pre-receive.sample\n",
      "./.git/hooks/prepare-commit-msg.sample\n",
      "./.git/hooks/post-update.sample\n",
      "./.git/hooks/pre-merge-commit.sample\n",
      "./.git/hooks/pre-applypatch.sample\n",
      "./.git/hooks/pre-push.sample\n",
      "./.git/hooks/update.sample\n",
      "./.git/hooks/push-to-checkout.sample\n",
      "./.git/refs/heads/hw7\n",
      "./.git/refs/heads/ha1\n",
      "./.git/refs/remotes/origin/HEAD\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import re\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"./Corona_NLP_train.csv\",encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "### Remove Unused Data from Dataframe\n",
    "\n",
    "The following is the function to drop all the data that will not be used from\n",
    "the dataframe.  Then the function is applied to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(p):\n",
    "    p.drop([\"UserName\",\"ScreenName\",\"Location\",\"TweetAt\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
       "1  advice Talk to your neighbours family to excha...            Positive\n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive\n",
       "3  My food stock is not the only one which is emp...            Positive\n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              11422\n",
       "Negative               9917\n",
       "Neutral                7713\n",
       "Extremely Positive     6624\n",
       "Extremely Negative     5481\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relabel Data to Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep(t):\n",
    "        d={\"Sentiment\":{'Positive':0,'Negative':1,\"Neutral\":2,\"Extremely Positive\":3,\"Extremely Negative\":4}}\n",
    "        t.replace(d,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  Sentiment\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...          2\n",
       "1  advice Talk to your neighbours family to excha...          0\n",
       "2  Coronavirus Australia: Woolworths to give elde...          0\n",
       "3  My food stock is not the only one which is emp...          0\n",
       "4  Me, ready to go at supermarket during the #COV...          4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Tokenization and Normalization\n",
    "\n",
    "Here, the data is tokenized and normalized with a `TweetTokenizer` and\n",
    "`WordNetLemmatizer` is used to complete the preprocessing of\n",
    "the corpus.  The initialized `PorterStemmer` is never used and I suspect that\n",
    "the author of this notebook discarded it in favor of `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweettoken = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect=[]\n",
    "def preprocess(t):\n",
    "    tee=re.sub('[^a-zA-Z]',\" \",t)\n",
    "    tee=tee.lower()\n",
    "    res=tweettoken.tokenize(tee)\n",
    "    for i in res:\n",
    "        if i in stopwords.words('english'):\n",
    "            res.remove(i)\n",
    "    rest=[]\n",
    "    for k in res:\n",
    "        rest.append(lemmatizer.lemmatize(k))\n",
    "    ret=\" \".join(rest)\n",
    "    collect.append(ret)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(41157):\n",
    "    preprocess(train[\"OriginalTweet\"].iloc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['menyrbie phil gahan chrisitv http co ifz fan pa http co xx ghgfzcc http co nlzdxno',\n",
       " 'advice talk your neighbour family exchange phone number create contact list phone number neighbour school employer chemist gp set online shopping account po adequate supply regular med not order',\n",
       " 'coronavirus australia woolworth give elderly disabled dedicated shopping hour amid covid outbreak http co binca vp p',\n",
       " 'food stock not only one is empty please panic will enough food everyone not take than you need stay calm stay safe covid france covid covid coronavirus confinement confinementotal confinementgeneral http t co zrlg z j',\n",
       " 'ready go supermarket covid outbreak because m paranoid because food stock litteraly empty the coronavirus a serious thing please panic cause shortage coronavirusfrance restezchezvous stayathome confinement http t co usmualq n']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bow Function\n",
    "\n",
    "This function uses `CountVectorizer` to convert an array of texts into a matrix\n",
    "of token counts.  In this case, it is initialized such that it will only account\n",
    "for the top 200 words by frequency in the corpus.  The resulting data is\n",
    "converted to an array and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(ll):\n",
    "    cv=CountVectorizer(max_features=200)\n",
    "    x=cv.fit_transform(ll).toarray()\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=bow(collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first count array from the first string in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification of the number of features/words being counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-Frequency Inverse-Document-Frequency Function\n",
    "\n",
    "This function uses the `TfidfVectorizer` as opposed to `CountVectorizer`\n",
    "(and is used later in this notebook to contrast results between the two data\n",
    "handling strategies).  `CountVectorizer` uses raw term frequencies to produce\n",
    "the resulting count matrix, whereas `TfidfVectorizer` produces a transformed\n",
    "(normalized) matrix from a raw count matrix.\n",
    "\n",
    "The resulting matrix produced by `TfidfVectorizer` scales down the weight of\n",
    "frequently occurring words/tokens since their impact on classification is less\n",
    "informative than tokens that occur less often.  In this case, the original\n",
    "author of this notebook chose to keep the default parameters except limiting the\n",
    "number of features to 4000.  This means that they chose to maintain\n",
    "inverse-document-frequency reweighting and smoothing.  The computation used by\n",
    "this algorithm for a\n",
    "term $t$ of a document $d$ has the mathematical formula\n",
    "$$ \\operatorname{tf-idf}(t,d) = \\operatorname{tf}(t,d) \\cdot \\log\n",
    "\\left[\\frac{1+n}{1+\\operatorname{df}(t)}\\right]+1 $$\n",
    "where $n$ is the total number of documents and $\\operatorname{df}(t)$ is the\n",
    "document frequency of the word/token/term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(xx):\n",
    "    cv=TfidfVectorizer(max_features=4000)\n",
    "    x=cv.fit_transform(xx).toarray()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=train[\"Sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 0, 2, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split matrices into random train and test subsets\n",
    "\n",
    "The `train_test_split` function takes a feature matrix and labels vector and\n",
    "splits it into separate randomized test and train subsets.  Here, the `x_train` is the\n",
    "training feature matrix and `y_train` is the vector labeling the training\n",
    "feature matrix.  Likewise, the `x_test` and `y_test` is the test portion of the\n",
    "randomized subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,x_test,y_train,y_test) = train_test_split(y,values, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Forest Classifier\n",
    "\n",
    "Not necessary for assignment.  The following blocks are associated with blocks\n",
    "#27-#32 of the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf=RandomForestClassifier(n_estimators=200,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4172983479105928"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1358,  592,  501,  338,  109],\n",
       "       [ 702, 1014,  448,  106,  243],\n",
       "       [ 477,  438,  913,   46,   45],\n",
       "       [ 653,  183,  162,  617,   28],\n",
       "       [ 287,  457,  148,   33,  392]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=rnd_clf.predict(x_test)\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4152575315840622\n",
      "0.41438289601554906\n",
      "0.41438289601554906\n",
      "0.41243926141885323\n",
      "0.41564625850340137\n",
      "0.4182701652089407\n",
      "0.41778425655976675\n"
     ]
    }
   ],
   "source": [
    "a=[400,500,600,700,800,900,1000]\n",
    "for i in a:\n",
    "    rnd_clf=RandomForestClassifier(n_estimators=i,random_state=42)\n",
    "    rnd_clf.fit(x_train,y_train)\n",
    "    t=rnd_clf.score(x_test,y_test)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multinomial Naive Bayes\n",
    "\n",
    "The following uses the randomized train data (`x_train` feature matrix and\n",
    "`y_train` label vector) to train a `MultinomialNB` classifier.  This run of the\n",
    "classifier uses raw word counts to train.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score for MultinomialNB with raw (not normalized) frequency counts\n",
    "\n",
    "This score is the mean accuracy of the prediction for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3825072886297376"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "This is a more granular measurement of the accuracy of the classification.\n",
    "Since we have five classes of documents, this confusion matrix is a 5x5 matrix\n",
    "showing the number of observations known to be in a particular class, but was\n",
    "predicted to be in a different class.\n",
    "\n",
    "Given the confusion matrix\n",
    "$$\\begin{pmatrix}\n",
    "C_{0,0} & C_{0,1} & C_{0,2} & C_{0,3} & C_{0,4}\\\\\n",
    "C_{1,0} & C_{1,1} & C_{1,2} & C_{1,3} & C_{1,4}\\\\\n",
    "C_{2,0} & C_{2,1} & C_{2,2} & C_{2,3} & C_{2,4}\\\\\n",
    "C_{3,0} & C_{3,1} & C_{3,2} & C_{3,3} & C_{3,4}\\\\\n",
    "C_{4,0} & C_{4,1} & C_{4,2} & C_{4,3} & C_{4,4}\n",
    "\\end{pmatrix},$$\n",
    "the value $C_{0,0}$ corresponds to the number of true $0$ or positive tweets\n",
    "that was predicted to be a $0$ or positive tweet.  Then $C_{0,1}$ corresponds\n",
    "to the number of true $0$ or positive tweets that was predicted to be a $1$ or\n",
    "negative tweet. This pattern is repeated for all possible combinations of true\n",
    "values against predicted values (the values were set in an earlier cell as\n",
    "follows: 0, positive; 1, negative; 2, neutral; 3, extremely positive; 4,\n",
    "extremely negative).  Notice that the entries on the main diagonal\n",
    "correspond to correct predictions and all other entries correspond to incorrect\n",
    "predictions.  Hence, we are looking to maximize the values on the main diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1001,  512,  709,  502,  174],\n",
       "       [ 586,  769,  601,  177,  380],\n",
       "       [ 406,  325, 1061,   62,   65],\n",
       "       [ 516,  153,  233,  670,   71],\n",
       "       [ 240,  384,  200,   58,  435]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(x_test)\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Normalized Feature Matrix\n",
    "\n",
    "This next section evaluates the accuracy of the same classifiers when given a\n",
    "normalized feature matrix.  I have provided an explanation of tf-idf normalization was outlined\n",
    "earlier in this notebook.  All of the following procedures are simply a\n",
    "repetition of what was done above, but with a normalized feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tfidf(collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,x_test,y_train,y_test) = train_test_split(y,values, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2937803692905734"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf=RandomForestClassifier(n_estimators=200,max_leaf_nodes=8,random_state=42)\n",
    "rnd_clf.fit(x_train,y_train)\n",
    "rnd_clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB Performance\n",
    "\n",
    "Notice that the score of the `MultinomialNB` improves significantly when moving from a\n",
    "raw count feature matrix to a normalized feature matrix (from 0.3825 to 0.4676,\n",
    "respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46763848396501456"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(x_train,y_train)\n",
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Comparison and Explanation\n",
    "\n",
    "The results from the confusion matrix coming from a normalized feature matrix (NFM) are\n",
    "interesting.  Looking carefully at both confusion matrices, we can see some patterns with how\n",
    "each predicted the different sentiments.\n",
    "\n",
    "The NFM confusion matrix shows that the predictions for positive and negative\n",
    "sentiment became much more accurate when compared to the raw count frequency\n",
    "matrix (RFM) confusion matrix.\n",
    "Another takeaway is that the NFM model was less likely to categorize any of the\n",
    "sentiments to neutral, or extreme classes.  We can make the conclusion that the\n",
    "NFM model was able to more accurately discern a binary classification (positive\n",
    "vs negative) than to the 5 classes available.  Indeed, a large percentage of\n",
    "true \"extremely positive\" classes were incorrectly predicted to be merely \"positive\" and\n",
    "similarly, a large percentage of true \"extremely positive\" classes were\n",
    "incorrectly predicted to be merely \"negative\".\n",
    "\n",
    "I would use the NFM model if I was not as concerned about the nuances between\n",
    "each class and just wanted a delineation of \"positive\" and \"negative\".  But\n",
    "if that were the case, I would just relabel all the extreme sentiments to its\n",
    "corresponding sentiments and reduce the number of classes to 3.  Also notice\n",
    "that the NFM model is disproportionately biased towards a positive sentiment.  Because of\n",
    "this, a researcher may want to use the RFM model over the NFM model, despite the\n",
    "increased accuracy of the NFM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2066,  491,  195,  131,   15],\n",
       "       [ 912, 1295,  186,   26,   94],\n",
       "       [ 767,  350,  774,   20,    8],\n",
       "       [1114,   78,   34,  415,    2],\n",
       "       [ 214,  807,   32,    2,  262]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(x_test)\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a71d39d660cde086a1906713d7782789105334eb77ec37352255bdee7f037f90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> b88f2fb72a7ce9626019c63504ef26c9b4694019
