{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using N-grams\n",
    "\n",
    "Project by Mark Kim (commenting and some code cleanup)\n",
    "\n",
    "As implemented by Namrata Kapoor\n",
    "On [Numpy\n",
    "Ninja](https://www.numpyninja.com/post/n-gram-and-its-use-in-text-generation)\n",
    "\n",
    "This is a simple program that generates sentences from an n-gram language model\n",
    "trained from a corpus of Donald Trump tweets.  First, we download the corpus\n",
    "from a [Kaggle](https://www.kaggle.com) dataset as a csv.  Then the csv is\n",
    "pre-processed by being entered into a dataframe, tokenized, and then is analyzed\n",
    "for n-grams (the code provided uses trigrams).  An Maximum Likelihood Estimate\n",
    "model is used with the pre-processed training set used to fit the model via\n",
    "supervised learning. Finally, the model is used along with a detokenizer to\n",
    "generate sentences based on the history of tweets sent by Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages used in this project\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize, download\n",
    "\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'need'),\n",
       " ('need', 'to'),\n",
       " ('to', 'book'),\n",
       " ('book', 'ticket'),\n",
       " ('ticket', 'to'),\n",
       " ('to', 'Australia')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a quick test of a toy corpus to extract bigrams\n",
    "\n",
    "text = [['I','need','to','book', 'ticket', 'to', 'Australia' ], \n",
    "['I', 'want', 'to' ,'read', 'a' ,'book', 'of' ,'Shakespeare']]\n",
    "list(bigrams(text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'want', 'to'),\n",
       " ('want', 'to', 'read'),\n",
       " ('to', 'read', 'a'),\n",
       " ('read', 'a', 'book'),\n",
       " ('a', 'book', 'of'),\n",
       " ('book', 'of', 'Shakespeare')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a test of the same toy corpus to extract trigrams\n",
    "\n",
    "list(ngrams(text[1], n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Trump Corpus\n",
    "\n",
    "This section of code loads the csv of Donald Trump tweets into a Pandas\n",
    "Dataframe.  The contents of the csv is tokenized and entered into a list of\n",
    "trump tweets (a list of lists).  This resulting corpus is then preprocessed to\n",
    "produce a tuple of an iterator over the text as ngrams and an iterator over the\n",
    "text as vocabulary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess csv of Donald Trump tweets\n",
    "\n",
    "df = pd.read_csv('./files/realdonaldtrump.csv')\n",
    "trump_corpus = list(df['content'].apply(word_tokenize))\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, trump_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1698308935</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 13:54:25</td>\n",
       "      <td>510</td>\n",
       "      <td>917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1701461182</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-04 20:00:10</td>\n",
       "      <td>34</td>\n",
       "      <td>267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1737479987</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 08:38:08</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1741160716</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 15:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1773561338</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 09:07:28</td>\n",
       "      <td>1375</td>\n",
       "      <td>1945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               link  \\\n",
       "0  1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1  1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "2  1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "3  1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "4  1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                             content                 date  \\\n",
       "0  Be sure to tune in and watch Donald Trump on L...  2009-05-04 13:54:25   \n",
       "1  Donald Trump will be appearing on The View tom...  2009-05-04 20:00:10   \n",
       "2  Donald Trump reads Top Ten Financial Tips on L...  2009-05-08 08:38:08   \n",
       "3  New Blog Post: Celebrity Apprentice Finale and...  2009-05-08 15:40:15   \n",
       "4  \"My persona will never be that of a wallflower...  2009-05-12 09:07:28   \n",
       "\n",
       "   retweets  favorites mentions hashtags  \n",
       "0       510        917      NaN      NaN  \n",
       "1        34        267      NaN      NaN  \n",
       "2        13         19      NaN      NaN  \n",
       "3        11         26      NaN      NaN  \n",
       "4      1375       1945      NaN      NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Maximum Likelihood Estimator N-gram Model\n",
    "\n",
    "Here, we instantiate the N-gram model and then train the model with the\n",
    "preprocessed data from the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train (fit) the model with preprocessed data\n",
    "\n",
    "trump_model = MLE(n)\n",
    "trump_model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Sentence Generation Function\n",
    "\n",
    "This next section of code instantiates a detokenizer that takes a list of\n",
    "strings generated by the model and returns the resulting string sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Sentence Generation function\n",
    "\n",
    "This generates a bunch of sentences from the generation function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   \n",
      "1   to meet with the Clinton Campaign Organized Potential VPs By Race And Gender: http: //bit.ly/fLEtcl\n",
      "2   job in the Polls . There will be on the President? The answer is--clean it with Presidential Privilege, I wonder what the next Miss USA pageant tomorrow night at Madison Square Garden I get more documentation\n",
      "3   them...but he just loose to Michelle - dummy, never even discussed this with them and being violent, and ALL!\n",
      "4   \"@ BackOnTrackUSA: Not only did Egypt destroy its nuclear capability until such time as illegal migrants coming through their country ......\n",
      "5   to the idea of Trump Attaché, @ lisarinna and @ BretBaier #WakeUpAmerica https: //www.donaldjtrump.com/iowa/caucus-finder/ …pic.twitter.com/1vjCHYYlzU\n",
      "6   //bit.ly/100qRgF\n",
      "7   with . Respect . #2020 because they think they can enter our Country, and totally showed their cards for everyone!\n",
      "8   CelebApprentice @ realDonaldTrump Please run for president please\"\n",
      "9   you both in very close, dummy Jon Stewart . She was awesome yesterday . http: //tl.gd/flvgbf\n",
      "10   give dollars for our next president! #PA10https: //secure.winred.com/scottperry/donate …\n",
      "11   !).\n",
      "12   . Massive seniority brings so much money with no security checks . Know everything you can watch my full & complete ....\n",
      "13   is worst in USA . Has no chance). Will fade fast . 184 miles as of today! pic.twitter.com/MsGVgODwVU\n",
      "14   .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(15):\n",
    "    print(i, \" \", generate_sent(trump_model, num_words=40, random_seed=random.randint(1,10000)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1da1dfe0ec3268b8d25684185f5f1b35e3a04c0e33abc4d4b7cb1822f1909834"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
