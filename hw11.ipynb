{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CSC 620 HA #11\n","\n","By: Mark Kim\n","\n","Adapted from: [Utham Bathoju](https://www.kaggle.com/code/uthamkanth/beginner-tf-idf-and-cosine-similarity-from-scratch/notebook)"]},{"cell_type":"markdown","metadata":{},"source":["This notebook examines doing a search of three toy documents for a particular\n","query.  In this case, the query is \"life learning\" on the following three documents:\n","\n","* **Document 1** : I want to start learning to charge something in life.\n","* **Document 2** : learning something about me no one else knows\n","* **Document 3** : Never stop learning\n","\n","This query is meant to be a free form query, which means that there are no\n","operators used or any other special words to link words in any particular way\n","(e.g. using *and*, *or*, etc.)\n","\n","The author begins with importing pandas, numpy and math."]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["import math\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["Define query and toy documents"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["#documents\n","doc1 = \"I want to start learning to charge something in life\"\n","doc2 = \"reading something about life no one else knows\"\n","doc3 = \"Never stop learning\"\n","#query string\n","query = \"life learning\""]},{"cell_type":"markdown","metadata":{},"source":["## Raw term frequency counts\n","\n","The following function was created simply to illustrate that term frequency is\n","not a good measure to base a search query from.  The terms have not been\n","normalized (e.g. capitalization removed, stopwords removed, etc.).  Furthermore,\n","we do not normalize to large term counts.  As we learned in lecture, a word that\n","appears 100 times should not weigh 100 times heavier in determining\n","\"importance\".  Lastly, for the data to be useful, we want to create\n","probabilities that terms appear, hence, raw counts will not be useful for us."]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["         Document  I  something  want  life  charge  in  learning  to  start\n","0  Term Frequency  1          1     1     1       1   1         1   2      1\n","         Document  something  about  reading  life  one  no  knows  else\n","0  Term Frequency          1      1        1     1    1   1      1     1\n","         Document  stop  Never  learning\n","0  Term Frequency     1      1         1\n"]}],"source":["def compute_tf(docs_list):\n","    for doc in docs_list:\n","        doc1_lst = doc.split(\" \")\n","        wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n","\n","        for token in doc1_lst:\n","            wordDict_1[token] +=  1\n","        df = pd.DataFrame([wordDict_1])\n","        idx = 0\n","        new_col = [\"Term Frequency\"]    \n","        df.insert(loc=idx, column='Document', value=new_col)\n","        print(df)\n","        \n","compute_tf([doc1, doc2, doc3])"]},{"cell_type":"markdown","metadata":{},"source":["## Normalized Term Frequency\n","\n","The next functions attempt at converting the raw term frequency counts to some\n","sort of probability so that we can determine the probability a term appears in a\n","document.  Very basic normalization is done here, where the capitalization of\n","words is removed."]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["        Document    I  something  want  life  charge   in  learning   to  \\\n","0  Normalized TF  0.1        0.1   0.1   0.1     0.1  0.1       0.1  0.2   \n","\n","   start  \n","0    0.1  \n","        Document  something  about  reading   life    one     no  knows   else\n","0  Normalized TF      0.125  0.125    0.125  0.125  0.125  0.125  0.125  0.125\n","        Document      stop     Never  learning\n","0  Normalized TF  0.333333  0.333333  0.333333\n"]}],"source":["#Normalized Term Frequency\n","def termFrequency(term, document):\n","    normalizeDocument = document.lower().split()\n","    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n","\n","def compute_normalizedtf(documents):\n","    tf_doc = []\n","    for txt in documents:\n","        sentence = txt.split()\n","        norm_tf= dict.fromkeys(set(sentence), 0)\n","        for word in sentence:\n","            norm_tf[word] = termFrequency(word, txt)\n","        tf_doc.append(norm_tf)\n","        df = pd.DataFrame([norm_tf])\n","        idx = 0\n","        new_col = [\"Normalized TF\"]    \n","        df.insert(loc=idx, column='Document', value=new_col)\n","        print(df)\n","    return tf_doc\n","\n","tf_doc = compute_normalizedtf([doc1, doc2, doc3])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Inverse Document Frequency (IDF)\n","\n","Here, we address the issue of large term counts having a disproportionate effect\n","in determining relevancy with respect to matching a particular query.  By\n","applying an inverse document frequency to the term frequency, we suppress the\n","weights of terms with high frequency counts.  Indeed, words that occur less\n","often are a better determinant of matching queries to a document.  Hence, we\n","apply the following formula to find the IDF:\n","$$ \\operatorname{idf_t} = \\log_{10}\\left(\\frac{N}{df_t}\\right) $$\n","where $N$ is the total number of documents in the corpus and $df_t$ is the\n","number of documents in which the term $t$ appears.\n","\n","In this case, it looks like the author uses Laplace smoothing, which explains\n","adding $1$ to the term count."]},{"cell_type":"markdown","metadata":{},"source":["# Inverse Document Frequency (IDF)\n","\n","* The main purpose of doing a search is to find out relevant documents matching the query. \n","* In Term Frequecy all terms are considered equally important. In fact certain terms that occur too frequently have little power in determining the relevance. \n","* We need a way to weigh down the effects of too frequently occurring terms. Also the terms that occur less in the document can be more relevant. \n","* We need a way to weigh up the effects of less frequently occurring terms. Logarithms helps us to solve this problem.Logarithms helps us to solve this problem.\n","\n","\n","Let us compute IDF for the term start\n","\n","IDF(start) = 1 + loge(Total Number Of Documents / Number Of Documents with term start in it)\n","\n","There are 3 documents in all = Document1, Document2, Document3\n","The term start appears in Document1\n","\n"," IDF(start) = 1 + loge(3 / 1)\n","            = 1 + 1.098726209\n","            = 2.098726209"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def inverseDocumentFrequency(term, allDocuments):\n","    numDocumentsWithThisTerm = 0\n","    for doc in range (0, len(allDocuments)):\n","        if term.lower() in allDocuments[doc].lower().split():\n","            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n"," \n","    if numDocumentsWithThisTerm > 0:\n","        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n","    else:\n","        return 1.0\n","    \n","def compute_idf(documents):\n","    idf_dict = {}\n","    for doc in documents:\n","        sentence = doc.split()\n","        for word in sentence:\n","            idf_dict[word] = inverseDocumentFrequency(word, documents)\n","    return idf_dict\n","idf_dict = compute_idf([doc1, doc2, doc3])\n","\n","compute_idf([doc1, doc2, doc3])"]},{"cell_type":"markdown","metadata":{},"source":["# 3.TF * IDF"]},{"cell_type":"markdown","metadata":{},"source":["Remember we are trying to find out relevant documents for the query: **life learning**\n","\n","* For each term in the query multiply its normalized term frequency with its IDF on each document. \n","* In Document1 for the term life the normalized term frequency is 0.1 and its IDF is 1.405465108. \n","* Multiplying them together we get 0.140550715 (0.1 * 1.405465108). \n","* \n","Given below is TF * IDF calculations for life and learning in all the documents."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# tf-idf score across all docs for the query string(\"life learning\")\n","def compute_tfidf_with_alldocs(documents , query):\n","    tf_idf = []\n","    index = 0\n","    query_tokens = query.split()\n","    df = pd.DataFrame(columns=['doc'] + query_tokens)\n","    for doc in documents:\n","        df['doc'] = np.arange(0 , len(documents))\n","        doc_num = tf_doc[index]\n","        sentence = doc.split()\n","        for word in sentence:\n","            for text in query_tokens:\n","                if(text == word):\n","                    idx = sentence.index(word)\n","                    tf_idf_score = doc_num[word] * idf_dict[word]\n","                    tf_idf.append(tf_idf_score)\n","                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n","        index += 1\n","    df.fillna(0 , axis=1, inplace=True)\n","    return tf_idf , df\n","            \n","documents = [doc1, doc2, doc3]\n","tf_idf , df = compute_tfidf_with_alldocs(documents , query)\n","print(df)"]},{"cell_type":"markdown","metadata":{},"source":["# 4.Vector Space Models and Representation  – Cosine Similarity\n","\n","The set of documents in a collection then is viewed as a set of vectors in a vector space. Each term will have its own axis. Using the formula given below we can find out the similarity between any two documents.\n","\n","* > Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n","* > Dot product (d1,d2) = d1[0] * d2[0] + d1[1] * d2[1] * … * d1[n] * d2[n]\n","* > ||d1|| = square root(d1[0]2 + d1[1]2 + ... + d1[n]2)\n","* > ||d2|| = square root(d2[0]2 + d2[1]2 + ... + d2[n]2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from IPython.display import Image\n","Image(\"../input/tfidf-kernel/cosinesimilarity.jpg\")"]},{"cell_type":"markdown","metadata":{},"source":["* Vectors deals only with numbers. In this example we are dealing with text documents. This was the reason why we used TF and IDF to convert text into numbers so that it can be represented by a vecto\n","\n","\n","The query entered by the user can also be represented as a vector. We will calculate the TF*IDF for the query"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Normalized TF for the query string(\"life learning\")\n","def compute_query_tf(query):\n","    query_norm_tf = {}\n","    tokens = query.split()\n","    for word in tokens:\n","        query_norm_tf[word] = termFrequency(word , query)\n","    return query_norm_tf\n","query_norm_tf = compute_query_tf(query)\n","print(query_norm_tf)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#idf score for the query string(\"life learning\")\n","def compute_query_idf(query):\n","    idf_dict_qry = {}\n","    sentence = query.split()\n","    documents = [doc1, doc2, doc3]\n","    for word in sentence:\n","        idf_dict_qry[word] = inverseDocumentFrequency(word ,documents)\n","    return idf_dict_qry\n","idf_dict_qry = compute_query_idf(query)\n","print(idf_dict_qry)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#tf-idf score for the query string(\"life learning\")\n","def compute_query_tfidf(query):\n","    tfidf_dict_qry = {}\n","    sentence = query.split()\n","    for word in sentence:\n","        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n","    return tfidf_dict_qry\n","tfidf_dict_qry = compute_query_tfidf(query)\n","print(tfidf_dict_qry)"]},{"cell_type":"markdown","metadata":{},"source":["Let us now calculate the cosine similarity of the query and Document1.\n","\n","Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n","\n","Dot product(Query, Document1) \n","     = ((0.702753576) * (0.140550715) + (0.702753576)*(0.140550715))\n","     = 0.197545035151\n","\n","||Query|| = sqrt((0.702753576)2 + (0.702753576)2) = 0.993843638185\n","\n","||Document1|| = sqrt((0.140550715)2 + (0.140550715)2) = 0.198768727354\n","\n","Cosine Similarity(Query, Document) = 0.197545035151 / (0.993843638185) * (0.198768727354)\n","                                        = 0.197545035151 / 0.197545035151\n","                                        = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n","\n","\"\"\"\n","Example : Dot roduct(Query, Document1) \n","\n","     life:\n","     = tfidf(life w.r.t query) * tfidf(life w.r.t Document1) +  / \n","     sqrt(tfidf(life w.r.t query)) * \n","     sqrt(tfidf(life w.r.t doc1))\n","     \n","     learning:\n","     =tfidf(learning w.r.t query) * tfidf(learning w.r.t Document1)/\n","     sqrt(tfidf(learning w.r.t query)) * \n","     sqrt(tfidf(learning w.r.t doc1))\n","\n","\"\"\"\n","def cosine_similarity(tfidf_dict_qry, df , query , doc_num):\n","    dot_product = 0\n","    qry_mod = 0\n","    doc_mod = 0\n","    tokens = query.split()\n","   \n","    for keyword in tokens:\n","        dot_product += tfidf_dict_qry[keyword] * df[keyword][df['doc'] == doc_num]\n","        #||Query||\n","        qry_mod += tfidf_dict_qry[keyword] * tfidf_dict_qry[keyword]\n","        #||Document||\n","        doc_mod += df[keyword][df['doc'] == doc_num] * df[keyword][df['doc'] == doc_num]\n","    qry_mod = np.sqrt(qry_mod)\n","    doc_mod = np.sqrt(doc_mod)\n","    #implement formula\n","    denominator = qry_mod * doc_mod\n","    cos_sim = dot_product/denominator\n","     \n","    return cos_sim\n","\n","from collections import Iterable\n","def flatten(lis):\n","     for item in lis:\n","        if isinstance(item, Iterable) and not isinstance(item, str):\n","             for x in flatten(item):\n","                yield x\n","        else:        \n","             yield item\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def rank_similarity_docs(data):\n","    cos_sim =[]\n","    for doc_num in range(0 , len(data)):\n","        cos_sim.append(cosine_similarity(tfidf_dict_qry, df , query , doc_num).tolist())\n","    return cos_sim\n","similarity_docs = rank_similarity_docs(documents)\n","doc_names = [\"Document1\", \"Document2\", \"Document3\"]\n","print(doc_names)\n","print(list(flatten(similarity_docs)))"]},{"cell_type":"markdown","metadata":{},"source":["* I plotted vector values for the query and documents in 2-dimensional space of life and learning. Document1 has the highest score of 1. This is not surprising as it has both the terms life and learning."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from IPython.display import Image\n","Image(\"../input/tfidf-kernel/cosinesimiarlity11.jpeg\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 ('csc620')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"a71d39d660cde086a1906713d7782789105334eb77ec37352255bdee7f037f90"}}},"nbformat":4,"nbformat_minor":4}
