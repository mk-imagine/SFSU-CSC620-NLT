{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CSC 620 HA #11\n","\n","By: Mark Kim\n","\n","Adapted from: [Utham Bathoju](https://www.kaggle.com/code/uthamkanth/beginner-tf-idf-and-cosine-similarity-from-scratch/notebook)"]},{"cell_type":"markdown","metadata":{},"source":["This notebook is broken up into three sections as follows:\n","\n","1. [Program_0](#program_0): Create a copy of the below notebook (or export it as\n","   python program), and add detailed description for each code block, in your\n","   own words.\n","2. [Program_1](#program_1): Revise the above program to replace the toy dataset\n","   with with a larger text dataset of your choice.\n","3. [Program_2](#program_2): Create a variation of Program_1 that uses only TF\n","   representation to compute the similarity between a query and document.\n","4. [Writeup](#Writeup): Submit a short write-up (1 to 2 paragraphs) that compares and contrasts the document rankings provided by Program_1 and Program_2 for the same 3 queries of your choice.  Reflect on why is the ranking different, which representation provides better ranking, etc."]},{"cell_type":"markdown","metadata":{},"source":["## Program_0\n","\n","Create a copy of the notebook and add detailed descriptions."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import math\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["Define query and toy documents"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#documents\n","doc1 = \"I want to start learning to charge something in life\"\n","doc2 = \"reading something about life no one else knows\"\n","doc3 = \"Never stop learning\"\n","#query string\n","query = \"life learning\""]},{"cell_type":"markdown","metadata":{},"source":["## Raw term frequency counts\n","\n","The following function was created simply to illustrate that term frequency is\n","not a good measure to base a search query from.  The terms have not been\n","normalized (e.g. capitalization removed, stopwords removed, etc.).  Furthermore,\n","we do not normalize to large term counts.  As we learned in lecture, a word that\n","appears 100 times should not weigh 100 times heavier in determining\n","\"importance\".  Lastly, for the data to be useful, we want to create\n","probabilities that terms appear, hence, raw counts will not be useful for us."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def compute_tf(docs_list):\n","    for doc in docs_list:\n","        doc1_lst = doc.split(\" \")\n","        wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n","\n","        for token in doc1_lst:\n","            wordDict_1[token] +=  1\n","        df = pd.DataFrame([wordDict_1])\n","        idx = 0\n","        new_col = [\"Term Frequency\"]    \n","        df.insert(loc=idx, column='Document', value=new_col)\n","        print(df)\n","        \n","compute_tf([doc1, doc2, doc3])"]},{"cell_type":"markdown","metadata":{},"source":["## Normalized Term Frequency\n","\n","The next functions attempt at converting the raw term frequency counts to some\n","sort of probability so that we can determine the probability a term appears in a\n","document.  Very basic normalization is done here, where the capitalization of\n","words is removed."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Normalized Term Frequency\n","def termFrequency(term, document):\n","    normalizeDocument = document.lower().split()\n","    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n","\n","def compute_normalizedtf(documents):\n","    tf_doc = []\n","    for txt in documents:\n","        sentence = txt.split()\n","        norm_tf= dict.fromkeys(set(sentence), 0)\n","        for word in sentence:\n","            norm_tf[word] = termFrequency(word, txt)\n","        tf_doc.append(norm_tf)\n","        df = pd.DataFrame([norm_tf])\n","        idx = 0\n","        new_col = [\"Normalized TF\"]    \n","        df.insert(loc=idx, column='Document', value=new_col)\n","        # print(df)\n","    return tf_doc\n","\n","tf_doc = compute_normalizedtf([doc1, doc2, doc3])"]},{"cell_type":"markdown","metadata":{},"source":["# Inverse Document Frequency (IDF)\n","\n","Here, we address the issue of large term counts having a disproportionate effect\n","in determining relevancy with respect to matching a particular query.  By\n","applying an inverse document frequency to the term frequency, we suppress the\n","weights of terms with high frequency counts.  Indeed, words that occur less\n","often are a better determinant of matching queries to a document.  Hence, we\n","apply the following formula to find the IDF:\n","$$ \\operatorname{idf_t} = \\log_{10}\\left(\\frac{N}{df_t}\\right) $$\n","where $N$ is the total number of documents in the corpus and $df_t$ is the\n","number of documents in which the term $t$ appears.\n","\n","In this case, it looks like the author increases the value for idf by $1$.  I am\n","not sure why the author does this since the formula doe not call for this\n","addition.  I have not removed the addition of $1$ because it does not change the\n","final results since this addition occurs in all cases."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def inverseDocumentFrequency(term, allDocuments):\n","    numDocumentsWithThisTerm = 0\n","    for doc in range (0, len(allDocuments)):\n","        if term.lower() in allDocuments[doc].lower().split():\n","            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n"," \n","    if numDocumentsWithThisTerm > 0:\n","        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n","    else:\n","        return 1.0\n","    \n","def compute_idf(documents):\n","    idf_dict = {}\n","    for doc in documents:\n","        sentence = doc.split()\n","        for word in sentence:\n","            idf_dict[word] = inverseDocumentFrequency(word, documents)\n","    return idf_dict\n","    \n","idf_dict = compute_idf([doc1, doc2, doc3])\n","\n","compute_idf([doc1, doc2, doc3])"]},{"cell_type":"markdown","metadata":{},"source":["Using the IDF values above, the author then calculates the IDF score for each\n","word in the query.  The following function compares the query words to the IDF\n","dictionary and term frequency dictionary for each document to find the TF-IDF\n","score for each word in each document."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# tf-idf score across all docs for the query string(\"life learning\")\n","def compute_tfidf_with_alldocs(documents , query):\n","    tf_idf = []\n","    index = 0\n","    query_tokens = query.split()\n","    df = pd.DataFrame(columns=['doc'] + query_tokens)\n","    for doc in documents:\n","        df['doc'] = np.arange(0 , len(documents))\n","        doc_num = tf_doc[index]\n","        sentence = doc.split()\n","        for word in sentence:\n","            for text in query_tokens:\n","                if(text == word):\n","                    idx = sentence.index(word)\n","                    tf_idf_score = doc_num[word] * idf_dict[word]\n","                    tf_idf.append(tf_idf_score)\n","                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n","        index += 1\n","    df.fillna(0 , axis=1, inplace=True)\n","    return tf_idf , df\n","            \n","documents = [doc1, doc2, doc3]\n","tf_idf , df = compute_tfidf_with_alldocs(documents , query)\n","print(df)"]},{"cell_type":"markdown","metadata":{},"source":["# Cosine Similarity\n","\n","The author takes an incremental approach here to calculate cosine similarities\n","between documents here.  They first calculate a normalized term frequency\n","dictionary, followed by calculating an IDF dictionary.  Once the term frequency\n","and IDF are calculated a final document weight vector can be calculated from the\n","results."]},{"cell_type":"markdown","metadata":{},"source":["### Term Frequency Function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Normalized TF for the query string(\"life learning\")\n","def compute_query_tf(query):\n","    query_norm_tf = {}\n","    tokens = query.split()\n","    for word in tokens:\n","        query_norm_tf[word] = termFrequency(word , query)\n","    return query_norm_tf\n","query_norm_tf = compute_query_tf(query)\n","print(query_norm_tf)"]},{"cell_type":"markdown","metadata":{},"source":["### IDF Function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#idf score for the query string(\"life learning\")\n","def compute_query_idf(query):\n","    idf_dict_qry = {}\n","    sentence = query.split()\n","    documents = [doc1, doc2, doc3]\n","    for word in sentence:\n","        idf_dict_qry[word] = inverseDocumentFrequency(word ,documents)\n","    return idf_dict_qry\n","idf_dict_qry = compute_query_idf(query)\n","print(idf_dict_qry)"]},{"cell_type":"markdown","metadata":{},"source":["### TF-IDF Function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#tf-idf score for the query string(\"life learning\")\n","def compute_query_tfidf(query):\n","    tfidf_dict_qry = {}\n","    sentence = query.split()\n","    for word in sentence:\n","        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n","    return tfidf_dict_qry\n","tfidf_dict_qry = compute_query_tfidf(query)\n","print(tfidf_dict_qry)"]},{"cell_type":"markdown","metadata":{},"source":["## Cosine Similarity Function\n","\n","Finally, all the above results can be combined to calculate cosine similarity\n","with the following formula:\n","$$ \\cos(\\vec{q},\\vec{d}) = \\frac{\\vec{q} \\cdot \\vec{d}}{\\lVert\\vec{q}\\rVert\n","\\lVert\\vec{d}\\rVert} =  \\frac{\\displaystyle\\sum_{i=1}^{\\lvert V\\rvert} q_i\n","d_i}{\\sqrt{\\displaystyle\\sum_{i=1}^{\\lvert V\\rvert}\n","q_i^2}\\sqrt{\\displaystyle\\sum_{i=1}^{\\lvert V\\rvert} di^2}}$$\n","\n","The `cosine_similarity` function is pretty self-explanatory.  It simply adds up\n","the products of the query and document TF-IDF scores, then divides them by the\n","product of the norms of each.\n","\n","I am not sure why the original author created a generator for the flatten\n","function since we don't really need to use lazy programming here: we are looking\n","to calculate all results."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n","\n","\"\"\"\n","Example : Dot roduct(Query, Document1) \n","\n","     life:\n","     = tfidf(life w.r.t query) * tfidf(life w.r.t Document1) +  / \n","     sqrt(tfidf(life w.r.t query)) * \n","     sqrt(tfidf(life w.r.t doc1))\n","     \n","     learning:\n","     =tfidf(learning w.r.t query) * tfidf(learning w.r.t Document1)/\n","     sqrt(tfidf(learning w.r.t query)) * \n","     sqrt(tfidf(learning w.r.t doc1))\n","\n","\"\"\"\n","def cosine_similarity(tfidf_dict_qry, df , query , doc_num):\n","    dot_product = 0\n","    qry_mod = 0\n","    doc_mod = 0\n","    tokens = query.split()\n","   \n","    for keyword in tokens:\n","        dot_product += tfidf_dict_qry[keyword] * df[keyword][df['doc'] == doc_num]\n","        #||Query||\n","        qry_mod += tfidf_dict_qry[keyword] * tfidf_dict_qry[keyword]\n","        #||Document||\n","        doc_mod += df[keyword][df['doc'] == doc_num] * df[keyword][df['doc'] == doc_num]\n","    qry_mod = np.sqrt(qry_mod)\n","    doc_mod = np.sqrt(doc_mod)\n","    #implement formula\n","    denominator = qry_mod * doc_mod\n","    cos_sim = dot_product/denominator\n","     \n","    return cos_sim\n","\n","from collections.abc import Iterable\n","def flatten(lis):\n","     for item in lis:\n","        if isinstance(item, Iterable) and not isinstance(item, str):\n","             for x in flatten(item):\n","                yield x\n","        else:        \n","             yield item\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def rank_similarity_docs(data):\n","    cos_sim =[]\n","    for doc_num in range(0 , len(data)):\n","        cos_sim.append(cosine_similarity(tfidf_dict_qry, df , query , doc_num).tolist())\n","    return cos_sim\n","similarity_docs = rank_similarity_docs(documents)\n","doc_names = [\"Document1\", \"Document2\", \"Document3\"]\n","print(doc_names)\n","print(list(flatten(similarity_docs)))"]},{"cell_type":"markdown","metadata":{},"source":["## Program_1"]},{"cell_type":"markdown","metadata":{},"source":["Load articles from The Onion, a satirical news outlet, then remove all rows that\n","contain invalid text."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["theonion = pd.read_csv(\"./file_archive/theonion.csv\")\n","theonion = theonion.dropna()\n","theonion[\"processed\"] = theonion[\"Content\"].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))"]},{"cell_type":"markdown","metadata":{},"source":["Grab a small sample from the dataset to reduce computation time."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["theonion = theonion.sample(frac=0.05, random_state=10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["theonion.head()"]},{"cell_type":"markdown","metadata":{},"source":["Extract just the content of each article and convert it to a list."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["onioncontentlist = theonion.loc[:, 'processed'].values.tolist()\n","len(onioncontentlist)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["oniontitlelist = theonion.loc[:, 'Title'].values.tolist()\n","oniontitlelist[:10]"]},{"cell_type":"markdown","metadata":{},"source":["Compute the normalized term frequency using the function created [above](#normalized-term-frequency)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_onion = compute_normalizedtf(onioncontentlist)\n","tf_onion"]},{"cell_type":"markdown","metadata":{},"source":["Compute the IDF using [compute_idf](#inverse-document-frequency-idf)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["idf_onion = compute_idf(onioncontentlist)"]},{"cell_type":"markdown","metadata":{},"source":["Pickle data so that computation does not need to be repeated."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import dill"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('./pickles/tf_onion.pkl', 'wb') as f:\n","    dill.dump(tf_onion, f)\n","\n","with open('./pickles/idf_onion.pkl', 'wb') as f:\n","    dill.dump(idf_onion, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('./pickles/tf_onion.pkl', 'rb') as f:\n","    tf_onion = dill.load(f)\n","\n","with open('./pickles/idf_onion.pkl', 'rb') as f:\n","    idf_onion = dill.load(f)"]},{"cell_type":"markdown","metadata":{},"source":["### Overload compute_tfidf_with_alldocs Function\n","\n","I overloaded the `compute_tfidf_with_alldocs` function to allow for a term\n","frequency dictionary and IDF dictionary to be passed in.  This removes the\n","necessity for closures."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_tfidf_with_alldocs(documents, query, tf_dict, idf_dict):\n","    tf_idf = []\n","    index = 0\n","    query_tokens = query.split()\n","    df = pd.DataFrame(columns=['doc'] + query_tokens)\n","    for doc in documents:\n","        df['doc'] = np.arange(0 , len(documents))\n","        doc_num = tf_dict[index]\n","        sentence = doc.split()\n","        for word in sentence:\n","            for text in query_tokens:\n","                if(text == word):\n","                    idx = sentence.index(word)\n","                    tf_idf_score = doc_num[word] * idf_dict[word]\n","                    tf_idf.append(tf_idf_score)\n","                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n","        index += 1\n","    df.fillna(0 , axis=1, inplace=True)\n","    return tf_idf , df"]},{"cell_type":"markdown","metadata":{},"source":["Create a new query for this new dataset and use the overloaded function to\n","produce the TF-IDF scores list and the dataframe of the TF-IDF scores for each\n","word in the query."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["query = \"president clinton confirmed ecuador is a south american nation\"\n","tf_idf_onion , df_onion = compute_tfidf_with_alldocs(onioncontentlist, query, tf_onion, idf_onion)\n","print(df_onion)"]},{"cell_type":"markdown","metadata":{},"source":["Use the previous functions to calculate the term frequencies and IDF for\n","the query."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["norm_tf_qry = compute_query_tf(query)\n","idf_dict_qry = compute_query_idf(query)\n","print(norm_tf_qry)\n","print(idf_dict_qry)"]},{"cell_type":"markdown","metadata":{},"source":["### Overload `compute_query_tfidf`\n","\n","Once again, I overload the `compute_query_tfidf` function to allow the query\n","term frequency and IDF dictionaries to be passed in."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_query_tfidf(query, norm_tf_qry, idf_dict_qry):\n","    tfidf_dict_qry = {}\n","    sentence = query.split()\n","    for word in sentence:\n","        tfidf_dict_qry[word] = norm_tf_qry[word] * idf_dict_qry[word]\n","    return tfidf_dict_qry"]},{"cell_type":"markdown","metadata":{},"source":["Run the function on the new query to find the TF-IDF for the query."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tfidf_dict_qry = compute_query_tfidf(query, norm_tf_qry, idf_dict_qry)\n","print(tfidf_dict_qry)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def rank_similarity_docs(data, df, query):\n","    cos_sim =[]\n","    for doc_num in range(0 , len(data)):\n","        cos_sim.append(cosine_similarity(tfidf_dict_qry, df , query , doc_num).tolist())\n","    return cos_sim"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_onion = rank_similarity_docs(onioncontentlist, df_onion, query)\n","similarity_onion = np.nan_to_num(np.array(similarity_onion).flatten())\n","theonion_cp = theonion.copy()\n","theonion_cp = theonion_cp.drop([\"processed\"], axis=1)\n","theonion_cp[\"similarity\"] = similarity_onion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["theonion_cp.sort_values(by=\"similarity\", ascending=False).head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["theonion_cp.loc[4007, \"Content\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["theonion_cp.loc[4920, \"Content\"]"]},{"cell_type":"markdown","metadata":{},"source":["## Program_2"]},{"cell_type":"markdown","metadata":{},"source":["Use only term frequency to compute similarities."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def compute_tf_with_alldocs(documents, query, tf_dict):\n","    tf = []\n","    index = 0\n","    query_tokens = query.split()\n","    df = pd.DataFrame(columns=['doc'] + query_tokens)\n","    for doc in documents:\n","        df['doc'] = np.arange(0 , len(documents))\n","        doc_num = tf_dict[index]\n","        sentence = doc.split()\n","        for word in sentence:\n","            for text in query_tokens:\n","                if(text == word):\n","                    idx = sentence.index(word)\n","                    tf_score = doc_num[word]\n","                    tf.append(tf_score)\n","                    df.iloc[index, df.columns.get_loc(word)] = tf_score\n","        index += 1\n","    df.fillna(0 , axis=1, inplace=True)\n","    return tf , df"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'onioncontentlist' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf_onion_p2 , df_onion_p2 \u001b[39m=\u001b[39m compute_tf_with_alldocs(onioncontentlist, query, tf_onion)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(df_onion_p2)\n","\u001b[0;31mNameError\u001b[0m: name 'onioncontentlist' is not defined"]}],"source":["tf_onion_p2 , df_onion_p2 = compute_tf_with_alldocs(onioncontentlist, query, tf_onion)\n","print(df_onion_p2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_query_tf(query, norm_tf_qry):\n","    tf_dict_qry = {}\n","    sentence = query.split()\n","    for word in sentence:\n","        tf_dict_qry[word] = norm_tf_qry[word]\n","    return tf_dict_qry"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_dict_qry = compute_query_tfidf(query, norm_tf_qry)\n","print(tf_dict_qry)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 ('csc620')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"a71d39d660cde086a1906713d7782789105334eb77ec37352255bdee7f037f90"}}},"nbformat":4,"nbformat_minor":4}
