{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC 620 - HW #14\n",
    "\n",
    "Student: Mark Kim\n",
    "\n",
    "Lesson Author: Jason Brownlee\n",
    "[How to Get Started with Deep Learning for Natural Language Processing](https://machinelearningmastery.com/crash-course-deep-learning-natural-language-processing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 - Deep Learning and Natural Language\n",
    "\n",
    "For this lesson you must research and list 10 impressive applications of deep\n",
    "learning methods in the field of natural language processing.\n",
    "\n",
    "1. [Qualifying Certainty in Radiology Reports through Deep Learning–Based Natural\n",
    "   Language\n",
    "   Processing](https://www-ncbi-nlm-nih-gov.jpllnet.sfsu.edu/pmc/articles/PMC8562739/)\n",
    "2. [A natural language processing and deep learning approach to identify child\n",
    "   abuse from pediatric electronic medical\n",
    "   records](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7909689/)\n",
    "3. [Deep Learning Natural Language Processing Successfully Predicts the\n",
    "   Cerebrovascular Cause of Transient Ischemic Attack-Like Presentations](https://www.ahajournals.org/doi/pdf/10.1161/STROKEAHA.118.024124)\n",
    "4. [Dynamic sign language translating system using deep learning and natural\n",
    "   language\n",
    "   processing](https://www.proquest.com/docview/2623612530?pq-origsite=primo)\n",
    "5. [NATURALPROOFS: Mathematical Theorem Proving in Natural\n",
    "   Language](https://arxiv.org/pdf/2104.01112.pdf)\n",
    "6. [Explaining neural activity in human listeners with deep learning via natural\n",
    "   language processing of narrative\n",
    "   text](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9596412/)\n",
    "7. [Identifying disaster-related tweets and their semantic, spatial andtemporal\n",
    "   context using deep learning, natural languageprocessing and spatial analysis:\n",
    "   a case study of Hurricane\n",
    "   Irma](https://www.tandfonline.com/doi/epdf/10.1080/17538947.2018.1563219)\n",
    "8. [Deep Learning-Based Natural Language Processing for Screening Psychiatric\n",
    "   Patients](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7874001/)\n",
    "9. [A natural language processing approach based on embedding deep learning from\n",
    "   heterogeneous compounds for quantitative structure–activity relationship\n",
    "   modeling](https://onlinelibrary.wiley.com/doi/10.1111/cbdd.13742)\n",
    "10. [Classifying social determinants of health from unstructured electronic health records using deep learning-based natural language processing](https://www.sciencedirect.com/science/article/abs/pii/S1532046421003130?via%3Dihub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 - Cleaning Text Data\n",
    "\n",
    "Your task is to locate a free classical book on the Project Gutenberg website,\n",
    "download the ASCII version of the book and tokenize the text and save the result\n",
    "to a new file.\n",
    "\n",
    "This section is relatively straightforward and does not really need explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "with open(\"ArtOfWar.txt\", 'rt') as f:\n",
    "  text = f.read()\n",
    "\n",
    "manualTokenized = text.lower().split()\n",
    "\n",
    "nltkTokenized = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'art', 'of', 'war,', 'by']\n",
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Art', 'of', 'War', ',']\n"
     ]
    }
   ],
   "source": [
    "print(manualTokenized[:10])\n",
    "print(nltkTokenized[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 - Bag-of-Words Model\n",
    "\n",
    "Your task in this lesson is to experiment with the scikit-learn and Keras\n",
    "methods for encoding small contrived text documents for the bag-of-words model.\n",
    "\n",
    "In this task (and subsequent tasks) I used the Project Gutenberg text above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of The Art of War, by Sun Tzŭ\\n\\nThis eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever.',\n",
       " 'You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww.gutenberg.org.',\n",
       " 'If you are not located in the United States, you\\nwill have to check the laws of the country where you are located before\\nusing this eBook.',\n",
       " 'Title: The Art of War\\n\\nAuthor: Sun Tzŭ\\n\\nTranslator: Lionel Giles\\n\\nRelease Date: May 1994 [eBook #132]\\n[Most recently updated: October 16, 2021]\\n\\nLanguage: English\\n\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK THE ART OF WAR ***\\n\\n\\n\\n\\nSun Tzŭ\\non\\nThe Art of War\\n\\nTHE OLDEST MILITARY TREATISE IN THE WORLD\\nTranslated from the Chinese with Introduction and Critical Notes\\n\\nBY\\nLIONEL GILES, M.A.',\n",
       " 'Assistant in the Department of Oriental Printed Books and MSS.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "nltkSentences = nltk.sent_tokenize(text)\n",
    "nltkSentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn\n",
    "\n",
    "Here, I vectorized the sentences in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['œufs', 'être', 'zenith', 'zenana', 'yüeh', 'yung_', 'yun', 'yuan_', 'yuan', 'yu_']\n",
      "[6.3975592  5.88673358 6.90838482 ... 8.41246222 8.41246222 8.41246222]\n",
      "(1, 6964)\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(nltkSentences)\n",
    "print(sorted(vectorizer.vocabulary_, key=vectorizer.vocabulary_.get, reverse=True)[:10])\n",
    "print(vectorizer.idf_)\n",
    "vector = vectorizer.transform([nltkSentences[0]])\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras\n",
    "\n",
    "The same was performed with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3866), ('of', 2151), ('to', 1718), ('and', 1485), ('in', 1189)]\n",
      "3312\n",
      "[('newsletter', 7102), ('subscribe', 7101), ('search', 7100), ('pg', 7099), ('paper', 7098)]\n",
      "[('the', 1867), ('of', 1357), ('to', 1190), ('and', 1062), ('in', 953)]\n",
      "[[0. 5. 4. ... 0. 0. 0.]\n",
      " [0. 2. 1. ... 0. 0. 0.]\n",
      " [0. 3. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 2. 0. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(nltkSentences)\n",
    "print(Counter(t.word_counts).most_common(5))\n",
    "print(t.document_count)\n",
    "print(Counter(t.word_index).most_common(5))\n",
    "print(Counter(t.word_docs).most_common(5))\n",
    "\n",
    "encoded_docs = t.texts_to_matrix(nltkSentences, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4: Word Embedding Representation\n",
    "\n",
    "Your task in this lesson is to train a word embedding using Gensim on a text\n",
    "document, such as a book from Project Gutenberg.\n",
    "\n",
    "First, I tokenized each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentTokenized = [nltk.word_tokenize(sentence) for sentence in nltkSentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this, I trained a Word2Vec model with the tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=7941, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentTokenized, min_count=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the first 15 \"words\" of the vocabulary is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'of', 'to', 'and', 'in', 'a', '’', 'is', \"''\", 'be', '``', 'that', '[']\n"
     ]
    }
   ],
   "source": [
    "words = model.wv.index_to_key\n",
    "print(words[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the vector for \"newsletter\" is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.41509445e-02  1.49208521e-02  7.79120345e-03  5.67935081e-03\n",
      "  2.33380822e-03 -2.06182525e-02  3.27301142e-03  1.83936525e-02\n",
      " -4.40958742e-04 -6.64643012e-03  2.01930925e-02 -1.33604538e-02\n",
      "  8.85030255e-03  1.71195623e-02  2.02103686e-02 -4.07641847e-03\n",
      "  4.47964156e-03 -6.89280313e-03 -4.98876534e-03 -2.78923344e-02\n",
      "  1.57653876e-02  1.59964990e-02  1.00263674e-02  1.73267408e-03\n",
      " -1.02725057e-02 -9.14873090e-05  2.73756214e-05 -8.87747109e-03\n",
      " -1.60669237e-02  7.13729952e-03  2.44184826e-02  3.17040714e-03\n",
      "  1.96864475e-02 -1.45607712e-02 -3.17949872e-03  1.43649261e-02\n",
      "  1.10767782e-02 -1.21024344e-02 -1.43542094e-02 -2.18408033e-02\n",
      "  1.09271659e-02 -1.43449083e-02 -3.34604667e-03 -1.00555085e-02\n",
      "  1.11531578e-02 -2.32433830e-03 -8.14381149e-03  9.90690780e-04\n",
      "  1.94936227e-02  8.64322856e-03  8.22429825e-03 -8.76567792e-03\n",
      " -4.07195231e-03  1.09597272e-03 -1.00022294e-02  2.74065562e-04\n",
      " -2.79109180e-03  6.26964262e-04 -1.17555428e-02  1.00423489e-02\n",
      "  1.18316906e-02 -1.10295331e-02  8.89694784e-03 -4.19057161e-03\n",
      " -1.68275312e-02 -7.71599065e-04 -8.97059310e-03  2.58286223e-02\n",
      " -1.69003382e-02  7.40348548e-03 -2.46991316e-04  1.60664320e-02\n",
      "  9.96680558e-03  1.61359989e-04  8.97642784e-03  9.03169904e-03\n",
      "  1.30604701e-02 -9.93930455e-03 -1.83843318e-02  6.00449787e-03\n",
      " -1.93098153e-03 -6.38584699e-03 -1.42780077e-02  9.25357547e-03\n",
      " -7.64288008e-04 -6.19302737e-03  1.28663285e-02  1.21540651e-02\n",
      "  1.27032464e-02  1.04976064e-02  9.96583793e-03 -1.93596457e-03\n",
      "  1.00769876e-02 -2.50685355e-03  1.08839208e-02  1.21025061e-02\n",
      "  1.43942675e-02 -1.77070238e-02  8.45146924e-03 -5.67152351e-03]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['newsletter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5: Learned Embedding\n",
    "\n",
    "Your task in this lesson is to design a small document classification problem\n",
    "with 10 documents of one sentence each and associated labels of positive and\n",
    "negative outcomes and to train a network with word embedding on these data. Note\n",
    "that each sentence will need to be padded to the same maximum length prior to\n",
    "training the model using the Keras pad_sequences() function.\n",
    "\n",
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Input, Encoding, and Padding (Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60, 97, 76, 92, 12], [97, 54, 92, 90, 7, 21, 42, 11, 92, 85, 40, 59], [54, 49, 76, 72], [50, 30, 19, 25, 93, 53, 6], [31, 71, 4, 24, 28, 15], [92, 22, 69, 50, 86, 71, 56, 22, 89], [31, 69, 70, 16, 2], [86, 91, 69, 97, 75, 24, 3, 62, 46, 86], [86, 86, 75, 22, 99, 48, 80, 76, 98], [48, 45, 7, 54, 92, 66]]\n",
      "[[60 97 76 92 12  0  0  0  0  0  0  0]\n",
      " [97 54 92 90  7 21 42 11 92 85 40 59]\n",
      " [54 49 76 72  0  0  0  0  0  0  0  0]\n",
      " [50 30 19 25 93 53  6  0  0  0  0  0]\n",
      " [31 71  4 24 28 15  0  0  0  0  0  0]\n",
      " [92 22 69 50 86 71 56 22 89  0  0  0]\n",
      " [31 69 70 16  2  0  0  0  0  0  0  0]\n",
      " [86 91 69 97 75 24  3 62 46 86  0  0]\n",
      " [86 86 75 22 99 48 80 76 98  0  0  0]\n",
      " [48 45  7 54 92 66  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "reviews = [\"Also, the meatballs were amazing.\", \n",
    "  \"The food and drinks here have fresh ingredients and are so tasty.\",\n",
    "  \"Definitely recommend this place!\",\n",
    "  \"Beautiful contemporary decor in an intimate ambience.\",\n",
    "  \"I was impressed with Prospect restaurant\",\n",
    "  \"And to my surprise, it was a major disappointment.\",\n",
    "  \"I will not be back.\",\n",
    "  \"It doesn't meet the expectations with your name on it.\",\n",
    "  \"It is sad to see restaurants going this way.\",\n",
    "  \"Horrible quality of food and service.\"\n",
    "  ]\n",
    "\n",
    "labels = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "\n",
    "encoded_docs = [one_hot(r, vocab_size) for r in reviews]\n",
    "\n",
    "print(encoded_docs)\n",
    "\n",
    "max_length = 12\n",
    "padded_reviews = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "print(padded_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Model for Binary Text Classification (with Sigmoid Function Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 12, 8)             800       \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 97        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 897\n",
      "Trainable params: 897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# define problem\n",
    "vocab_size = 100\n",
    "max_length = 12\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(padded_reviews, labels, epochs=50, verbose=0)\n",
    "loss, acc = model.evaluate(padded_reviews, labels, verbose=0)\n",
    "print('Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6: Classifying Text with a CNN\n",
    "\n",
    "Your task in this lesson is to research the use of the Embeddings + CNN combination of deep learning methods for text classification and report on examples or best practices for configuring this model, such as the number of layers, kernel size, vocabulary size and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup of a CNN for text classification typically involves feeding a word\n",
    "embedding into a 1-D CNN.  Almost all the examples I have seen (anecdotally on\n",
    "various blogs *and* in research papers) use around $32$ filters with kernel\n",
    "sizes ranging between 3 and 8.  In Yoon Kim's paper \"Convolutional Neural\n",
    "Networks for Sentence Classification\", the following configuration seems to work\n",
    "as a good starting point:\n",
    "- Activation Function: RELU (Rectified Linear)\n",
    "- Kernel Sizes: 3, 4, 5\n",
    "- Filter Count: 100\n",
    "- Dropout Rate: 0.5\n",
    "- Weight Regularization (L2): 3\n",
    "- Batch Size: 50\n",
    "- Update Rule: Adadelta\n",
    "Nevertheless, the kernel size and kernel count should be tuned for each\n",
    "particular problem.\n",
    "\n",
    "From further research, I found that many people use a filter count of 32 with\n",
    "kernel sizes ranging from 3-8.  From cursory research, I have not found anyone\n",
    "attempting to use any other activation function other than RELU, but the author\n",
    "of this assignment mentions that tanh and other linear activation functions may\n",
    "produce good results.\n",
    "\n",
    "Beyond the CNN, it seems that 1-max pooling typically outperforms other types of\n",
    "pooling.  Nevertheless, it is always good to experiment with many different\n",
    "settings to pinpoint the best strategy for one's particular application.\n",
    "\n",
    "Finally, there are other approaches such as Character-Level CNNs and using\n",
    "Deeper CNNs for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOPS!  I thought I was supposed to explain the math and theory of CNNs.\n",
    "\n",
    "I stopped my explanation below after I figured out my error.\n",
    "\n",
    "From my research, I have found that using CNNs for text classification typically\n",
    "consist of the embedding (the word vectors of dimension $d$) with a 1D\n",
    "convolution layer of size $k$.  This applies a convolution filter (or kernel) as\n",
    "a sliding window across the word sequence, which is a dot-product between a\n",
    "concatenation of the word vectors in the window and a weight vector $\\vec{u}$.\n",
    "This dot-product results in a scalar value, say $r_i$ for each $i$-th window.\n",
    "Typically, one applies many filters, which yields a weight matrix $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('csc620')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a71d39d660cde086a1906713d7782789105334eb77ec37352255bdee7f037f90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
