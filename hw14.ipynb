{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC 620 - HW #14\n",
    "\n",
    "Student: Mark Kim\n",
    "\n",
    "Lesson Author: Jason Brownlee\n",
    "[How to Get Started with Deep Learning for Natural Language Processing](https://machinelearningmastery.com/crash-course-deep-learning-natural-language-processing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 - Deep Learning and Natural Language\n",
    "\n",
    "For this lesson you must research and list 10 impressive applications of deep\n",
    "learning methods in the field of natural language processing.\n",
    "\n",
    "1. [Qualifying Certainty in Radiology Reports through Deep Learning–Based Natural\n",
    "   Language\n",
    "   Processing](https://www-ncbi-nlm-nih-gov.jpllnet.sfsu.edu/pmc/articles/PMC8562739/)\n",
    "2. [A natural language processing and deep learning approach to identify child\n",
    "   abuse from pediatric electronic medical\n",
    "   records](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7909689/)\n",
    "3. [Deep Learning Natural Language Processing Successfully Predicts the\n",
    "   Cerebrovascular Cause of Transient Ischemic Attack-Like Presentations](https://www.ahajournals.org/doi/pdf/10.1161/STROKEAHA.118.024124)\n",
    "4. [Dynamic sign language translating system using deep learning and natural\n",
    "   language\n",
    "   processing](https://www.proquest.com/docview/2623612530?pq-origsite=primo)\n",
    "5. [NATURALPROOFS: Mathematical Theorem Proving in Natural\n",
    "   Language](https://arxiv.org/pdf/2104.01112.pdf)\n",
    "6. [Explaining neural activity in human listeners with deep learning via natural\n",
    "   language processing of narrative\n",
    "   text](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9596412/)\n",
    "7. [Identifying disaster-related tweets and their semantic, spatial andtemporal\n",
    "   context using deep learning, natural languageprocessing and spatial analysis:\n",
    "   a case study of Hurricane\n",
    "   Irma](https://www.tandfonline.com/doi/epdf/10.1080/17538947.2018.1563219)\n",
    "8. [Deep Learning-Based Natural Language Processing for Screening Psychiatric\n",
    "   Patients](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7874001/)\n",
    "9. [A natural language processing approach based on embedding deep learning from\n",
    "   heterogeneous compounds for quantitative structure–activity relationship\n",
    "   modeling](https://onlinelibrary.wiley.com/doi/10.1111/cbdd.13742)\n",
    "10. [Classifying social determinants of health from unstructured electronic health records using deep learning-based natural language processing](https://www.sciencedirect.com/science/article/abs/pii/S1532046421003130?via%3Dihub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 - Cleaning Text Data\n",
    "\n",
    "Your task is to locate a free classical book on the Project Gutenberg website, download the ASCII version of the book and tokenize the text and save the result to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "with open(\"ArtOfWar.txt\", 'rt') as f:\n",
    "  text = f.read()\n",
    "\n",
    "manualTokenized = text.lower().split()\n",
    "\n",
    "nltkTokenized = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'art', 'of', 'war,', 'by']\n",
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Art', 'of', 'War', ',']\n"
     ]
    }
   ],
   "source": [
    "print(manualTokenized[:10])\n",
    "print(nltkTokenized[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 - Bag-of-Words Model\n",
    "\n",
    "Your task in this lesson is to experiment with the scikit-learn and Keras methods for encoding small contrived text documents for the bag-of-words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of The Art of War, by Sun Tzŭ\\n\\nThis eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever.',\n",
       " 'You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww.gutenberg.org.',\n",
       " 'If you are not located in the United States, you\\nwill have to check the laws of the country where you are located before\\nusing this eBook.',\n",
       " 'Title: The Art of War\\n\\nAuthor: Sun Tzŭ\\n\\nTranslator: Lionel Giles\\n\\nRelease Date: May 1994 [eBook #132]\\n[Most recently updated: October 16, 2021]\\n\\nLanguage: English\\n\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK THE ART OF WAR ***\\n\\n\\n\\n\\nSun Tzŭ\\non\\nThe Art of War\\n\\nTHE OLDEST MILITARY TREATISE IN THE WORLD\\nTranslated from the Chinese with Introduction and Critical Notes\\n\\nBY\\nLIONEL GILES, M.A.',\n",
       " 'Assistant in the Department of Oriental Printed Books and MSS.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "nltkSentences = nltk.sent_tokenize(text)\n",
    "nltkSentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['œufs', 'être', 'zenith', 'zenana', 'yüeh', 'yung_', 'yun', 'yuan_', 'yuan', 'yu_']\n",
      "[6.3975592  5.88673358 6.90838482 ... 8.41246222 8.41246222 8.41246222]\n",
      "(1, 6964)\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(nltkSentences)\n",
    "print(sorted(vectorizer.vocabulary_, key=vectorizer.vocabulary_.get, reverse=True)[:10])\n",
    "print(vectorizer.idf_)\n",
    "vector = vectorizer.transform([nltkSentences[0]])\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3866), ('of', 2151), ('to', 1718), ('and', 1485), ('in', 1189)]\n",
      "3312\n",
      "[('newsletter', 7102), ('subscribe', 7101), ('search', 7100), ('pg', 7099), ('paper', 7098)]\n",
      "[('the', 1867), ('of', 1357), ('to', 1190), ('and', 1062), ('in', 953)]\n",
      "[[0. 5. 4. ... 0. 0. 0.]\n",
      " [0. 2. 1. ... 0. 0. 0.]\n",
      " [0. 3. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 2. 0. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(nltkSentences)\n",
    "print(Counter(t.word_counts).most_common(5))\n",
    "print(t.document_count)\n",
    "print(Counter(t.word_index).most_common(5))\n",
    "print(Counter(t.word_docs).most_common(5))\n",
    "\n",
    "encoded_docs = t.texts_to_matrix(nltkSentences, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4: Word Embedding Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('csc620')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a71d39d660cde086a1906713d7782789105334eb77ec37352255bdee7f037f90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
